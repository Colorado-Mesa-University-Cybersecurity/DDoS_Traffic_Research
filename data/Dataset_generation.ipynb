{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Generation\n",
    "\n",
    "Here we will generate 2 batches of datasets for our multi-class classification experiments. \n",
    "\n",
    "First we produce our baseline datasets containing most of the features present in CIC_DDoS2019, and then we produce our time-based feature datasets, each containing only the 25 time-based features as well as a label"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by importing relavent libraries, setting a seed for reproducibility, and by printing out the versions of the libraries we are using for reproducibility."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os, platform, pprint, sys\r\n",
    "import matplotlib as mpl\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "seed: int = 14\r\n",
    "\r\n",
    "# set up pretty printer for easier data evaluation\r\n",
    "pretty = pprint.PrettyPrinter(indent=4, width=30).pprint\r\n",
    "\r\n",
    "print(\r\n",
    "    f'''\r\n",
    "    python:\\t{platform.python_version()}\r\n",
    "\r\n",
    "    \\tmatplotlib:\\t{mpl.__version__}\r\n",
    "    \\tnumpy:\\t\\t{np.__version__}\r\n",
    "    \\tpandas:\\t\\t{pd.__version__}\r\n",
    "    '''\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "    python:\t3.7.10\n",
      "\n",
      "    \tmatplotlib:\t3.3.4\n",
      "    \tnumpy:\t\t1.20.3\n",
      "    \tpandas:\t\t1.2.5\n",
      "    \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminaries\n",
    "\n",
    "Next, we do some preliminary set up. We list the data files we will be using and a list of new column names for the datasets that is more readable and understandable."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data_set_1: list = [\r\n",
    "    'DrDoS_DNS.csv'   , 'DrDoS_LDAP.csv'    ,\r\n",
    "    'DrDoS_MSSQL.csv' , 'DrDoS_NetBIOS.csv' ,\r\n",
    "    'DrDoS_NTP.csv'   , 'DrDoS_SNMP.csv'    ,\r\n",
    "    'DrDoS_SSDP.csv'  , 'DrDoS_UDP.csv'     ,\r\n",
    "    'Syn.csv'         , 'TFTP.csv'          ,\r\n",
    "    'UDPLag.csv'      ,    \r\n",
    "]\r\n",
    "    \r\n",
    "data_set_2: list = [\r\n",
    "    'LDAP.csv'    ,'MSSQL.csv'   ,\r\n",
    "    'NetBIOS.csv' ,'Portmap.csv' ,   \r\n",
    "    'Syn.csv'     ,'UDP.csv'     ,\r\n",
    "    'UDPLag.csv'  ,\r\n",
    "]\r\n",
    "\r\n",
    "data_set: list = data_set_1 + data_set_2\r\n",
    "\r\n",
    "\r\n",
    "# a list of DDoS attack types with indicies that map to the indicies of data_set\r\n",
    "data_location: list = [ \r\n",
    "    'DNS' , 'LDAP'  , 'MSSQL', 'NetBIOS', 'NTP'    , 'SNMP'   , 'SSDP', 'UDP', 'Syn'   , \r\n",
    "    'TFTP', 'UDPLag', 'LDAP' , 'MSSQL'  , 'NetBIOS', 'Portmap', 'Syn' , 'UDP', 'UDPLag',\r\n",
    "]\r\n",
    "\r\n",
    "\r\n",
    "# standardized column names for our data\r\n",
    "new_column_names: dict = {\r\n",
    "    'Unnamed: 0'                :'Unnamed'                  , 'Flow ID'                     :'Flow ID'                      ,\r\n",
    "    ' Source IP'                :'Source IP'                , ' Source Port'                :'Source Port'                  ,\r\n",
    "    ' Destination IP'           :'Destination IP'           , ' Destination Port'           :'Destination Port'             ,\r\n",
    "    ' Protocol'                 :'Protocol'                 , ' Total Length of Bwd Packets':'Total Length of Bwd Packets'  ,     \r\n",
    "    ' Flow Duration'            :'Flow Duration'            , ' Total Fwd Packets'          :'Total Fwd Packets'            , \r\n",
    "    ' Total Backward Packets'   :'Total Backward Packets'   , 'Total Length of Fwd Packets' :'Total Length of Fwd Packets'  ,\r\n",
    "    ' Timestamp'                :'Timestamp'                , ' Init_Win_bytes_backward'    :'Init Win bytes backward'      ,\r\n",
    "    ' Fwd Packet Length Max'    :'Fwd Packet Length Max'    , ' Fwd Packet Length Min'      :'Fwd Packet Length Min'        ,\r\n",
    "    ' Fwd Packet Length Mean'   :'Fwd Packet Length Mean'   , ' Fwd Packet Length Std'      :'Fwd Packet Length Std'        ,\r\n",
    "    'Bwd Packet Length Max'     :'Bwd Packet Length Max'    , ' Bwd Packet Length Min'      :'Bwd Packet Length Min'        ,\r\n",
    "    ' Bwd Packet Length Mean'   :'Bwd Packet Length Mean'   , ' Bwd Packet Length Std'      :'Bwd Packet Length Std'        ,\r\n",
    "    'Flow Bytes/s'              :'Flow Bytes/s'             , ' Flow Packets/s'             :'Flow Packets/s'               ,\r\n",
    "    ' Flow IAT Mean'            :'Flow IAT Mean'            , ' Flow IAT Std'               :'Flow IAT Std'                 ,\r\n",
    "    ' Flow IAT Max'             :'Flow IAT Max'             , ' Flow IAT Min'               :'Flow IAT Min'                 ,\r\n",
    "    'Fwd IAT Total'             :'Fwd IAT Total'            , ' Fwd IAT Mean'               :'Fwd IAT Mean'                 ,\r\n",
    "    ' Fwd IAT Std'              :'Fwd IAT Std'              , ' Fwd IAT Max'                :'Fwd IAT Max'                  ,\r\n",
    "    ' Fwd IAT Min'              :'Fwd IAT Min'              , 'Bwd IAT Total'               :'Bwd IAT Total'                ,    \r\n",
    "    ' Bwd IAT Mean'             :'Bwd IAT Mean'             , ' Bwd IAT Std'                :'Bwd IAT Std'                  ,\r\n",
    "    ' Bwd IAT Max'              :'Bwd IAT Max'              , ' Bwd IAT Min'                :'Bwd IAT Min'                  ,\r\n",
    "    'Fwd PSH Flags'             :'Fwd PSH Flags'            , ' Bwd PSH Flags'              :'Bwd PSH Flags'                , \r\n",
    "    ' Fwd URG Flags'            :'Fwd URG Flags'            , ' Bwd URG Flags'              :'Bwd URG Flags'                ,\r\n",
    "    ' Fwd Header Length'        :'Fwd Header Length'        , ' Bwd Header Length'          :'Bwd Header Length'            , \r\n",
    "    'Fwd Packets/s'             :'Fwd Packets/s'            , ' Bwd Packets/s'              :'Bwd Packets/s'                , \r\n",
    "    ' Min Packet Length'        :'Min Packet Length'        , ' Max Packet Length'          :'Max Packet Length'            , \r\n",
    "    ' Packet Length Mean'       :'Packet Length Mean'       , ' Packet Length Std'          :'Packet Length Std'            , \r\n",
    "    ' Packet Length Variance'   :'Packet Length Variance'   , 'FIN Flag Count'              :'FIN Flag Count'               ,\r\n",
    "    ' SYN Flag Count'           :'SYN Flag Count'           , ' RST Flag Count'             :'RST Flag Count'               ,\r\n",
    "    ' PSH Flag Count'           :'PSH Flag Count'           , ' ACK Flag Count'             :'ACK Flag Count'               , \r\n",
    "    ' URG Flag Count'           :'URG Flag Count'           , ' CWE Flag Count'             :'CWE Flag Count'               , \r\n",
    "    ' ECE Flag Count'           :'ECE Flag Count'           , ' Down/Up Ratio'              :'Down/Up Ratio'                ,\r\n",
    "    ' Average Packet Size'      :'Average Packet Size'      , ' Avg Fwd Segment Size'       :'Avg Fwd Segment Size'         ,\r\n",
    "    ' Avg Bwd Segment Size'     :'Avg Bwd Segment Size'     , ' Fwd Header Length.1'        :'Fwd Header Length.1'          , \r\n",
    "    'Fwd Avg Bytes/Bulk'        :'Fwd Avg Bytes/Bulk'       , ' Inbound'                    :'Inbound'                      , \r\n",
    "    ' Fwd Avg Packets/Bulk'     :'Fwd Avg Packets/Bulk'     , ' Fwd Avg Bulk Rate'          :'Fwd Avg Bulk Rate'            , \r\n",
    "    ' Bwd Avg Bytes/Bulk'       :'Bwd Avg Bytes/Bulk'       , ' Bwd Avg Packets/Bulk'       :'Bwd Avg Packets/Bulk'         ,\r\n",
    "    'Bwd Avg Bulk Rate'         :'Bwd Avg Bulk Rate'        , 'Subflow Fwd Packets'         :'Subflow Fwd Packets'          ,\r\n",
    "    ' Subflow Fwd Bytes'        :'Subflow Fwd Bytes'        , ' Subflow Bwd Packets'        :'Subflow Bwd Packets'          ,\r\n",
    "    ' Subflow Bwd Bytes'        :'Subflow Bwd Bytes'        , 'Init_Win_bytes_forward'      :'Init Win bytes forward'       ,\r\n",
    "    ' act_data_pkt_fwd'         :'act data pkt fwd'         , ' min_seg_size_forward'       :'min seg size forward'         ,     \r\n",
    "    'Active Mean'               :'Active Mean'              , ' Active Std'                 :'Active Std'                   ,\r\n",
    "    ' Active Max'               :'Active Max'               , ' Active Min'                 :'Active Min'                   , \r\n",
    "    'Idle Mean'                 :'Idle Mean'                , ' Idle Std'                   :'Idle Std'                     ,\r\n",
    "    ' Idle Max'                 :'Idle Max'                 , ' Idle Min'                   :'Idle Min'                     ,\r\n",
    "    'SimillarHTTP'              :'SimillarHTTP'             , ' Label'                      :'Label'                        ,\r\n",
    "}\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def get_file_path(directory: str):\r\n",
    "    '''\r\n",
    "        Closure that will return a function that returns the filepath to the directory given to the closure\r\n",
    "    '''\r\n",
    "\r\n",
    "    def func(file: str) -> str:\r\n",
    "        return os.path.join(directory, file)\r\n",
    "\r\n",
    "    return func\r\n",
    "\r\n",
    "\r\n",
    "# locations of the data files relative to current directory\r\n",
    "data_path_1: str = './original/01-12/'\r\n",
    "data_path_2: str = './original/03-11/'\r\n",
    "\r\n",
    "\r\n",
    "# use the get_file_path closure to create a function that will return the path to a file\r\n",
    "file_path_1 = get_file_path(data_path_1)\r\n",
    "file_path_2 = get_file_path(data_path_2)\r\n",
    "\r\n",
    "\r\n",
    "# a list of all complete filepaths relative to current directory with indicies mapped to the indicies of data_set\r\n",
    "file_set: list = list(map(file_path_1, data_set_1))\r\n",
    "file_set.extend(list(map(file_path_2, data_set_2)))\r\n",
    "\r\n",
    "\r\n",
    "print(f'We will be cleaning {len(file_set)} files:')\r\n",
    "print(f'Benign samples will be grabbed from each dataset and saved separately\\n')\r\n",
    "pretty(file_set)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "We will be cleaning 18 files:\n",
      "Benign samples will be grabbed from each dataset and saved separately\n",
      "\n",
      "[   './original/01-12/DrDoS_DNS.csv',\n",
      "    './original/01-12/DrDoS_LDAP.csv',\n",
      "    './original/01-12/DrDoS_MSSQL.csv',\n",
      "    './original/01-12/DrDoS_NetBIOS.csv',\n",
      "    './original/01-12/DrDoS_NTP.csv',\n",
      "    './original/01-12/DrDoS_SNMP.csv',\n",
      "    './original/01-12/DrDoS_SSDP.csv',\n",
      "    './original/01-12/DrDoS_UDP.csv',\n",
      "    './original/01-12/Syn.csv',\n",
      "    './original/01-12/TFTP.csv',\n",
      "    './original/01-12/UDPLag.csv',\n",
      "    './original/03-11/LDAP.csv',\n",
      "    './original/03-11/MSSQL.csv',\n",
      "    './original/03-11/NetBIOS.csv',\n",
      "    './original/03-11/Portmap.csv',\n",
      "    './original/03-11/Syn.csv',\n",
      "    './original/03-11/UDP.csv',\n",
      "    './original/03-11/UDPLag.csv']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have our file paths, we set up a list of features to prune during our preprocessing phase"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# prune is a list of all features we know we don't want to use\r\n",
    "# Unnamed is eliminated because it is un-labeled and we cannot verify what it qualities of the data if describes\r\n",
    "# Fwd Header Length.1 is eliminated because it is a duplicate\r\n",
    "# all the other features are eliminated because they are string values and cannot be used for classification\r\n",
    "prune: list = [\r\n",
    "    'Fwd Header Length.1',\r\n",
    "    'Unnamed',\r\n",
    "    'Source Port',\r\n",
    "    'Destination Port',\r\n",
    "    'Flow ID',\r\n",
    "    'Source IP',\r\n",
    "    'Destination IP',\r\n",
    "    'Timestamp',\r\n",
    "    'SimillarHTTP'\r\n",
    "] "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Maranhao et al. found in their study 'Tensor based framework for Distributed Denial of Service attack detection' that nine features were filled with only 0 values for every data collection in the dataset. Since an empty column of zeros will not contribute to the model's performance, we will remove those columns."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# toPrune is a list of features with empty columns of 0s\r\n",
    "toPrune: list = [\r\n",
    "    'Fwd URG Flags',\r\n",
    "    'Bwd URG Flags',\r\n",
    "    'Fwd PSH Flags',\r\n",
    "    'Fwd Avg Bytes/Bulk',\r\n",
    "    'Fwd Avg Packets/Bulk',\r\n",
    "    'Fwd Avg Bulk Rate',\r\n",
    "    'Bwd Avg Bytes/Bulk',\r\n",
    "    'Bwd Avg Packets/Bulk',\r\n",
    "    'Bwd Avg Bulk Rate'\r\n",
    "]\r\n",
    "\r\n",
    "for i in toPrune:\r\n",
    "    if i not in prune:\r\n",
    "        prune.append(i)\r\n",
    "\r\n",
    "print(f'We will be pruning {len(prune)} features')\r\n",
    "for i, x in enumerate(prune):\r\n",
    "    print(f'\\t{i+1}:\\t{x}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "We will be pruning 18 features\n",
      "\t1:\tFwd Header Length.1\n",
      "\t2:\tUnnamed\n",
      "\t3:\tSource Port\n",
      "\t4:\tDestination Port\n",
      "\t5:\tFlow ID\n",
      "\t6:\tSource IP\n",
      "\t7:\tDestination IP\n",
      "\t8:\tTimestamp\n",
      "\t9:\tSimillarHTTP\n",
      "\t10:\tFwd URG Flags\n",
      "\t11:\tBwd URG Flags\n",
      "\t12:\tFwd PSH Flags\n",
      "\t13:\tFwd Avg Bytes/Bulk\n",
      "\t14:\tFwd Avg Packets/Bulk\n",
      "\t15:\tFwd Avg Bulk Rate\n",
      "\t16:\tBwd Avg Bytes/Bulk\n",
      "\t17:\tBwd Avg Packets/Bulk\n",
      "\t18:\tBwd Avg Bulk Rate\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing and Data Cleaning\n",
    "\n",
    "Now that the preliminaries are done, we start processing the data. First we define some functions to load and clean the data, then we combine the data into dataframes based on their DDoS attack type. We keep the data manageable by sampling it down to sets of a million samples, using our seed to ensure that the results are reproducible."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\r\n",
    "    '''\r\n",
    "        Function will take a dataframe and remove the values from prune \r\n",
    "        Inf values will also be removed from Flow Bytes/s and Flow Packets/s\r\n",
    "        once appropriate rows and columns have been removed, we will return\r\n",
    "        the dataframe with the appropriate values\r\n",
    "    '''\r\n",
    "\r\n",
    "    # remove the features in the prune list    \r\n",
    "    for col in prune:\r\n",
    "        if col in df.columns:\r\n",
    "            df.drop(columns=[col], inplace=True)\r\n",
    "            \r\n",
    "    \r\n",
    "    # drop missing values/NaN etc.\r\n",
    "    df.dropna(inplace=True)\r\n",
    "\r\n",
    "    \r\n",
    "    # Search through dataframe for any Infinite or NaN values in various forms that were not picked up previously\r\n",
    "    invalid_values: list = [\r\n",
    "        np.inf, np.nan, 'Infinity', 'inf', 'NaN', 'nan'\r\n",
    "    ]\r\n",
    "    \r\n",
    "    for col in df.columns:\r\n",
    "        for value in invalid_values:\r\n",
    "            indexNames = df[df[col] == value].index\r\n",
    "            if not indexNames.empty:\r\n",
    "                print(f'deleting {len(indexNames)} rows with Infinity in column {col}')\r\n",
    "                df.drop(indexNames, inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "    # Standardize the contents of the Label column\r\n",
    "    df = df.replace( ['DrDoS_DNS'], 'DNS')\r\n",
    "    df = df.replace( ['DrDoS_LDAP'], 'LDAP')\r\n",
    "    df = df.replace( ['DrDoS_MSSQL'], 'MSSQL')\r\n",
    "    df = df.replace( ['DrDoS_NetBIOS'], 'NetBIOS')\r\n",
    "    df = df.replace( ['DrDoS_NTP'], 'NTP')\r\n",
    "    df = df.replace( ['DrDoS_SNMP'], 'SNMP')\r\n",
    "    df = df.replace( ['DrDoS_SSDP'], 'SSDP')\r\n",
    "    df = df.replace( ['DrDoS_UDP'], 'UDP')\r\n",
    "    df = df.replace( ['UDP-lag'], 'UDPLag')\r\n",
    "\r\n",
    "    \r\n",
    "    return df\r\n",
    "\r\n",
    "\r\n",
    "def load_data(filePath: str) -> tuple:\r\n",
    "    '''\r\n",
    "        Loads the Dataset from the given filepath and caches it for quick access in the future\r\n",
    "        Function will only work when filepath is a .csv file\r\n",
    "        After the data is loaded, the benign samples are split and saved in a list\r\n",
    "        the malicious samples are split and saved in a dictionary of lists indexed by attack type\r\n",
    "        only the top million malicious samples are kept\r\n",
    "    '''\r\n",
    "\r\n",
    "    # slice off the ./CSV/ from the filePath\r\n",
    "    if filePath[0] == '.' and filePath[1] == '/':\r\n",
    "        filePathClean: str = filePath[11::]\r\n",
    "        pickleDump: str = f'./cache/{filePathClean}.pickle'\r\n",
    "    else:\r\n",
    "        pickleDump: str = f'./cache/{filePath}.pickle'\r\n",
    "    \r\n",
    "    print(f'Loading Dataset: {filePath}')\r\n",
    "    print(f'\\tTo Dataset Cache: {pickleDump}\\n')\r\n",
    "    \r\n",
    "    # check if data already exists within cache\r\n",
    "    if os.path.exists(pickleDump):\r\n",
    "        df = pd.read_pickle(pickleDump)\r\n",
    "        \r\n",
    "    # if not, load data and clean it before caching it\r\n",
    "    else:\r\n",
    "        df = pd.read_csv(filePath, low_memory=True)\r\n",
    "        df.to_pickle(pickleDump)\r\n",
    "\r\n",
    "    df = df.rename(columns=new_column_names)\r\n",
    "    \r\n",
    "    # split the data into benign and malicious samples, keeping only the top 1 milliion\r\n",
    "    # (+ 200 thousand to replace samples removed by cleaning) malicious samples\r\n",
    "    benignSamples = df[df['Label'] == 'BENIGN']\r\n",
    "    maliciousSamples = df[df['Label'] != 'BENIGN']\r\n",
    "    if maliciousSamples.shape[0] > 1200000:\r\n",
    "        maliciousSamples = maliciousSamples.sample(n=1200000, random_state=seed)\r\n",
    "\r\n",
    "    # we remove the WebDDoS attacks because they are mixed in with other attacks\r\n",
    "    # and the dataset only provides less than 500 WebDDoS samples\r\n",
    "    # so we cannot do anything meaningful with them at the scale of our other experiments\r\n",
    "    maliciousSamples = maliciousSamples[maliciousSamples['Label'] != 'WebDDoS']\r\n",
    "\r\n",
    "    print(f'\\tLoaded {df.shape[0]} Samples as {benignSamples.shape[0]} Benign samples and {maliciousSamples.shape[0]} Malicious samples\\n')\r\n",
    "\r\n",
    "    return (benignSamples, maliciousSamples)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# set up a dictionary to hold all the malicious samples\r\n",
    "malicious_dict: dict = {}\r\n",
    "for i in range(len(data_location)):\r\n",
    "    malicious_dict[data_location[i]] = []\r\n",
    "\r\n",
    "\r\n",
    "# load the data and save the samples in a dictionary or list for further processing\r\n",
    "for i in range(len(data_set)):\r\n",
    "    benignSamples, maliciousSamples = load_data(file_set[i])\r\n",
    "    benignSamples = clean_data(benignSamples)\r\n",
    "    maliciousSamples = clean_data(maliciousSamples)\r\n",
    "    print()\r\n",
    "\r\n",
    "    if i == 0:\r\n",
    "        benign_list: list = [benignSamples]\r\n",
    "    else:\r\n",
    "        benign_list.append(benignSamples)\r\n",
    "\r\n",
    "    malicious_dict[data_location[i]].append(maliciousSamples)\r\n",
    "\r\n",
    "\r\n",
    "# save the benign samples as a single dataframe\r\n",
    "benign_df: pd.DataFrame = pd.concat(benign_list, ignore_index=True)\r\n",
    "print(f'Benign Samples: {benign_df.shape[0]}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Dataset: ./original/01-12/DrDoS_DNS.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/DrDoS_DNS.csv.pickle\n",
      "\n",
      "\tLoaded 5074413 Samples as 3402 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 26 rows with Infinity in column Flow Bytes/s\n",
      "deleting 38356 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/DrDoS_LDAP.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/DrDoS_LDAP.csv.pickle\n",
      "\n",
      "\tLoaded 2181542 Samples as 1612 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 10 rows with Infinity in column Flow Bytes/s\n",
      "deleting 21382 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/DrDoS_MSSQL.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/DrDoS_MSSQL.csv.pickle\n",
      "\n",
      "\tLoaded 4524498 Samples as 2006 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 9 rows with Infinity in column Flow Bytes/s\n",
      "deleting 33524 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/DrDoS_NetBIOS.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/DrDoS_NetBIOS.csv.pickle\n",
      "\n",
      "\tLoaded 4094986 Samples as 1707 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 18 rows with Infinity in column Flow Bytes/s\n",
      "deleting 38082 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/DrDoS_NTP.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/DrDoS_NTP.csv.pickle\n",
      "\n",
      "\tLoaded 1217007 Samples as 14365 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 66 rows with Infinity in column Flow Bytes/s\n",
      "deleting 6935 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/DrDoS_SNMP.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/DrDoS_SNMP.csv.pickle\n",
      "\n",
      "\tLoaded 5161377 Samples as 1507 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 9 rows with Infinity in column Flow Bytes/s\n",
      "deleting 2514 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/DrDoS_SSDP.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/DrDoS_SSDP.csv.pickle\n",
      "\n",
      "\tLoaded 2611374 Samples as 763 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 7 rows with Infinity in column Flow Bytes/s\n",
      "deleting 19387 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/DrDoS_UDP.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/DrDoS_UDP.csv.pickle\n",
      "\n",
      "\tLoaded 3136802 Samples as 2157 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 24 rows with Infinity in column Flow Bytes/s\n",
      "deleting 15432 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/Syn.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/Syn.csv.pickle\n",
      "\n",
      "\tLoaded 1582681 Samples as 392 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 8 rows with Infinity in column Flow Bytes/s\n",
      "deleting 30 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/TFTP.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/TFTP.csv.pickle\n",
      "\n",
      "\tLoaded 20107827 Samples as 25247 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 127 rows with Infinity in column Flow Bytes/s\n",
      "deleting 33223 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/UDPLag.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/UDPLag.csv.pickle\n",
      "\n",
      "\tLoaded 370605 Samples as 3705 Benign samples and 366461 Malicious samples\n",
      "\n",
      "deleting 21 rows with Infinity in column Flow Bytes/s\n",
      "deleting 250 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/03-11/LDAP.csv\n",
      "\tTo Dataset Cache: ./cache/03-11/LDAP.csv.pickle\n",
      "\n",
      "\tLoaded 2113234 Samples as 5124 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 58 rows with Infinity in column Flow Bytes/s\n",
      "deleting 30607 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/03-11/MSSQL.csv\n",
      "\tTo Dataset Cache: ./cache/03-11/MSSQL.csv.pickle\n",
      "\n",
      "\tLoaded 5775786 Samples as 2794 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 20 rows with Infinity in column Flow Bytes/s\n",
      "deleting 41891 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/03-11/NetBIOS.csv\n",
      "\tTo Dataset Cache: ./cache/03-11/NetBIOS.csv.pickle\n",
      "\n",
      "\tLoaded 3455899 Samples as 1321 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 21 rows with Infinity in column Flow Bytes/s\n",
      "deleting 45375 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/03-11/Portmap.csv\n",
      "\tTo Dataset Cache: ./cache/03-11/Portmap.csv.pickle\n",
      "\n",
      "\tLoaded 191694 Samples as 4734 Benign samples and 186960 Malicious samples\n",
      "\n",
      "deleting 36 rows with Infinity in column Flow Bytes/s\n",
      "deleting 9763 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/03-11/Syn.csv\n",
      "\tTo Dataset Cache: ./cache/03-11/Syn.csv.pickle\n",
      "\n",
      "\tLoaded 4320541 Samples as 35790 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 385 rows with Infinity in column Flow Bytes/s\n",
      "deleting 79056 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/03-11/UDP.csv\n",
      "\tTo Dataset Cache: ./cache/03-11/UDP.csv.pickle\n",
      "\n",
      "\tLoaded 3782206 Samples as 3134 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 47 rows with Infinity in column Flow Bytes/s\n",
      "deleting 24642 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/03-11/UDPLag.csv\n",
      "\tTo Dataset Cache: ./cache/03-11/UDPLag.csv.pickle\n",
      "\n",
      "\tLoaded 725165 Samples as 4068 Benign samples and 721097 Malicious samples\n",
      "\n",
      "deleting 50 rows with Infinity in column Flow Bytes/s\n",
      "deleting 50649 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Benign Samples: 112731\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "for key in malicious_dict.keys():\r\n",
    "    for entry in malicious_dict[key]:\r\n",
    "        print(key, ':', entry.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DNS : (1161642, 70)\n",
      "LDAP : (1178617, 70)\n",
      "LDAP : (1169393, 70)\n",
      "MSSQL : (1166476, 70)\n",
      "MSSQL : (1158109, 70)\n",
      "NetBIOS : (1161918, 70)\n",
      "NetBIOS : (1154624, 70)\n",
      "NTP : (1193062, 70)\n",
      "SNMP : (1197484, 70)\n",
      "SSDP : (1180613, 70)\n",
      "UDP : (1184567, 70)\n",
      "UDP : (1175358, 70)\n",
      "Syn : (1046564, 70)\n",
      "Syn : (1120924, 70)\n",
      "TFTP : (1166188, 70)\n",
      "UDPLag : (330079, 70)\n",
      "UDPLag : (670447, 70)\n",
      "Portmap : (177197, 70)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "attack_samples: dict = {}\r\n",
    "for key in malicious_dict.keys():\r\n",
    "    ddos_samples: list = []\r\n",
    "    for df in malicious_dict[key]:\r\n",
    "        ddos_samples.append(df[df['Label'] == key])\r\n",
    "\r\n",
    "    new_df = pd.concat(ddos_samples, ignore_index=True)\r\n",
    "    \r\n",
    "    if new_df.shape[0] > 1000000: \r\n",
    "        new_df = new_df.sample(n=1000000, random_state=seed)\r\n",
    "    \r\n",
    "    attack_samples[key] = new_df\r\n",
    "    \r\n",
    "\r\n",
    "print('Benign', ':', benign_df.shape)\r\n",
    "for key in attack_samples.keys():\r\n",
    "    print(key, ':', attack_samples[key].shape)\r\n",
    "    print(f'\\tto file: ./prepared/single/{key}.csv')\r\n",
    "    attack_samples[key].to_csv(f'./prepared/single/{key}.csv', index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Benign : (112731, 70)\n",
      "DNS : (1000000, 70)\n",
      "\tto file: ./prepared/single/DNS.csv\n",
      "LDAP : (1000000, 70)\n",
      "\tto file: ./prepared/single/LDAP.csv\n",
      "MSSQL : (1000000, 70)\n",
      "\tto file: ./prepared/single/MSSQL.csv\n",
      "NetBIOS : (1000000, 70)\n",
      "\tto file: ./prepared/single/NetBIOS.csv\n",
      "NTP : (1000000, 70)\n",
      "\tto file: ./prepared/single/NTP.csv\n",
      "SNMP : (1000000, 70)\n",
      "\tto file: ./prepared/single/SNMP.csv\n",
      "SSDP : (1000000, 70)\n",
      "\tto file: ./prepared/single/SSDP.csv\n",
      "UDP : (1000000, 70)\n",
      "\tto file: ./prepared/single/UDP.csv\n",
      "Syn : (1000000, 70)\n",
      "\tto file: ./prepared/single/Syn.csv\n",
      "TFTP : (1000000, 70)\n",
      "\tto file: ./prepared/single/TFTP.csv\n",
      "UDPLag : (331952, 70)\n",
      "\tto file: ./prepared/single/UDPLag.csv\n",
      "Portmap : (177197, 70)\n",
      "\tto file: ./prepared/single/Portmap.csv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print('Benign', ':', benign_df.shape)\r\n",
    "print(f'\\tto file: ./prepared/single/BENIGN.csv')\r\n",
    "benign_df.to_csv(f'./prepared/single/BENIGN.csv', index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Benign : (112731, 70)\n",
      "\tto file: ./prepared/single/BENIGN.csv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "assert False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline dataset generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have all of our data in 13 dataframes, we can begin to create our datasets. We will form 13 datasets for one-vs-all multi-class classification.\r\n",
    "\r\n",
    "\r\n",
    "The first dataset will be the Benign vs DDoS dataset. It will be a 50/50 split of the Benign vs DDoS samples. The DDoS samples will be equal parts of each DDoS attack type. Since we have around 112 thousand benign samples, we will use 112,000 benign samples and 112,000 samples of each DDoS attack type. \r\n",
    "\r\n",
    "\r\n",
    "Datasets 2-13 will each be one of the DDoS attack types (except WebDDoS since it has less samples) vs a basket of all the other DDoS attack types and the benign samples. Each dataset will have double the number of available samples for that attack type.\r\n",
    "\r\n",
    "Datasets 14-25 will be each of the DDoS attack types against Benign Samples. Each dataset will have double the number of available benign samples"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# First we make the first dataset, benign vs DDoS. It will be a 50/50 split between \r\n",
    "# benign and DDoS samples where the DDoS samples are chosen equally from a pool of\r\n",
    "# all the DDoS attack types.\r\n",
    "\r\n",
    "total_benign = benign_df.shape[0]\r\n",
    "total_each_attack_type = int(total_benign/12)\r\n",
    "\r\n",
    "DDoS_list = []\r\n",
    "for key in attack_samples.keys():\r\n",
    "    DDoS_list.append(attack_samples[key].sample(n=total_each_attack_type, random_state=seed))\r\n",
    "\r\n",
    "ddos_df = pd.concat(DDoS_list, ignore_index=True)\r\n",
    "\r\n",
    "to_replace = list(attack_samples.keys())\r\n",
    "# to_replace.append('UDP-lag')\r\n",
    "\r\n",
    "ddos_df.replace(to_replace=to_replace, value=\"DDOS\", inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "Benign_vs_DDoS = pd.concat([benign_df, ddos_df], ignore_index=True)\r\n",
    "print(f'Benign vs DDoS: {Benign_vs_DDoS.shape}')\r\n",
    "\r\n",
    "Benign_vs_DDoS.to_csv(\"./prepared/baseline/Benign_vs_DDoS.csv\", index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Benign vs DDoS: (225459, 70)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Here we make the Many vs Many dataset that includes benign samples.\r\n",
    "# We use the same number of benign and ddos samples from each ddos attack type.\r\n",
    "total_benign = benign_df.shape[0]\r\n",
    "total_each_attack_type = total_benign\r\n",
    "\r\n",
    "DDoS_list = []\r\n",
    "for key in attack_samples.keys():\r\n",
    "    DDoS_list.append(attack_samples[key].sample(n=total_each_attack_type, random_state=seed))\r\n",
    "\r\n",
    "ddos_df = pd.concat(DDoS_list, ignore_index=True)\r\n",
    "\r\n",
    "# to_replace = list(attack_samples.keys())\r\n",
    "# # to_replace.append('UDP-lag')\r\n",
    "\r\n",
    "# ddos_df.replace(to_replace=to_replace, value=\"DDOS\", inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "Benign_Many_vs_Many = pd.concat([benign_df, ddos_df], ignore_index=True)\r\n",
    "print(f'Benign Many vs Many: {Benign_Many_vs_Many.shape}')\r\n",
    "\r\n",
    "Benign_Many_vs_Many.to_csv(\"./prepared/baseline/Benign_Many_vs_Many.csv\", index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Benign Many vs Many: (1465503, 70)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# here we generate attack vs basket datasets. one for each attack type\r\n",
    "for key in attack_samples.keys():\r\n",
    "    \r\n",
    "    total_attacks = attack_samples[key].shape[0]\r\n",
    "    total_each_other_type = int(total_attacks / 12)\r\n",
    "    \r\n",
    "    other_type_list = []\r\n",
    "    for attack in attack_samples.keys():\r\n",
    "        if attack != key:\r\n",
    "            other_type_list.append(attack_samples[attack].sample(n=total_each_other_type, random_state=seed))\r\n",
    "    \r\n",
    "    other_type_list.append(benign_df.sample(n=total_each_other_type, random_state=seed))\r\n",
    "\r\n",
    "    other_df = pd.concat(other_type_list, ignore_index=True)\r\n",
    "\r\n",
    "    to_replace = list(attack_samples.keys())\r\n",
    "    to_replace.append('BENIGN')\r\n",
    "\r\n",
    "    other_df.replace(to_replace=to_replace, value=f'NOT{key}', inplace=True)\r\n",
    "\r\n",
    "    attack_df = attack_samples[key]\r\n",
    "\r\n",
    "    Attack_vs_all = pd.concat([attack_df, other_df], ignore_index=True)\r\n",
    "    print(f'{key} vs all: {Attack_vs_all.shape}')\r\n",
    "\r\n",
    "    Attack_vs_all.to_csv(f'./prepared/baseline/{key}_vs_all.csv', index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DNS vs all: (1999996, 70)\n",
      "LDAP vs all: (1999996, 70)\n",
      "MSSQL vs all: (1999996, 70)\n",
      "NetBIOS vs all: (1999996, 70)\n",
      "NTP vs all: (1999996, 70)\n",
      "SNMP vs all: (1999996, 70)\n",
      "SSDP vs all: (1999996, 70)\n",
      "UDP vs all: (1999996, 70)\n",
      "Syn vs all: (1999996, 70)\n",
      "TFTP vs all: (1999996, 70)\n",
      "UDPLag vs all: (663896, 70)\n",
      "Portmap vs all: (354389, 70)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# here we generate the many vs many dataset without benign samples\r\n",
    "total_each_attack_type: int = 10000000\r\n",
    "for key in attack_samples.keys():\r\n",
    "    if total_each_attack_type > attack_samples[key].shape[0]:\r\n",
    "        total_each_attack_type = attack_samples[key].shape[0]\r\n",
    "\r\n",
    "attack_list = []\r\n",
    "for attack in attack_samples.keys():\r\n",
    "    attack_list.append(attack_samples[attack].sample(n=total_each_attack_type, random_state=seed))\r\n",
    "\r\n",
    "Attacks_Many_vs_Many = pd.concat(attack_list, ignore_index=True)\r\n",
    "print(f'Attacks Many vs Many: {Attacks_Many_vs_Many.shape}')\r\n",
    "\r\n",
    "Attacks_Many_vs_Many.to_csv(f'./prepared/baseline/Attacks_Many_vs_Many.csv', index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Attacks Many vs Many: (2126364, 70)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# here we generate attack vs benign datasets. one for each attack type\r\n",
    "for key in attack_samples.keys():\r\n",
    "    \r\n",
    "    total_benign = benign_df.shape[0]\r\n",
    "\r\n",
    "    attack_df = attack_samples[key].sample(n=total_benign)\r\n",
    "\r\n",
    "    Attack_vs_Benign = pd.concat([attack_df, benign_df], ignore_index=True)\r\n",
    "    print(f'{key} vs Benign: {Attack_vs_Benign.shape}')\r\n",
    "\r\n",
    "    Attack_vs_Benign.to_csv(f'./prepared/baseline/{key}_vs_benign.csv', index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DNS vs Benign: (225462, 70)\n",
      "LDAP vs Benign: (225462, 70)\n",
      "MSSQL vs Benign: (225462, 70)\n",
      "NetBIOS vs Benign: (225462, 70)\n",
      "NTP vs Benign: (225462, 70)\n",
      "SNMP vs Benign: (225462, 70)\n",
      "SSDP vs Benign: (225462, 70)\n",
      "UDP vs Benign: (225462, 70)\n",
      "Syn vs Benign: (225462, 70)\n",
      "TFTP vs Benign: (225462, 70)\n",
      "UDPLag vs Benign: (225462, 70)\n",
      "Portmap vs Benign: (225462, 70)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Benign_vs_DDoS.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(225459, 70)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# here we generate attack vs basket datasets. one for each attack type without benign samples\r\n",
    "for key in attack_samples.keys():\r\n",
    "    \r\n",
    "    total_attacks = attack_samples[key].shape[0]\r\n",
    "    total_each_other_type = int(total_attacks / 11)\r\n",
    "    \r\n",
    "    other_type_list = []\r\n",
    "    for attack in attack_samples.keys():\r\n",
    "        if attack != key:\r\n",
    "            other_type_list.append(attack_samples[attack].sample(n=total_each_other_type, random_state=seed))\r\n",
    "    \r\n",
    "\r\n",
    "    other_df = pd.concat(other_type_list, ignore_index=True)\r\n",
    "\r\n",
    "    to_replace = list(attack_samples.keys())\r\n",
    "    to_replace.append('BENIGN')\r\n",
    "\r\n",
    "    other_df.replace(to_replace=to_replace, value=f'DDOS', inplace=True)\r\n",
    "\r\n",
    "    attack_df = attack_samples[key]\r\n",
    "\r\n",
    "    Attack_vs_DDoS = pd.concat([attack_df, other_df], ignore_index=True)\r\n",
    "    print(f'{key} vs DDoS: {Attack_vs_DDoS.shape}')\r\n",
    "\r\n",
    "    Attack_vs_DDoS.to_csv(f'./prepared/baseline/{key}_vs_ddos.csv', index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DNS vs DDoS: (1999999, 70)\n",
      "LDAP vs DDoS: (1999999, 70)\n",
      "MSSQL vs DDoS: (1999999, 70)\n",
      "NetBIOS vs DDoS: (1999999, 70)\n",
      "NTP vs DDoS: (1999999, 70)\n",
      "SNMP vs DDoS: (1999999, 70)\n",
      "SSDP vs DDoS: (1999999, 70)\n",
      "UDP vs DDoS: (1999999, 70)\n",
      "Syn vs DDoS: (1999999, 70)\n",
      "TFTP vs DDoS: (1999999, 70)\n",
      "UDPLag vs DDoS: (663899, 70)\n",
      "Portmap vs DDoS: (354385, 70)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time-Based Dataset Generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since one of our research directions is investigating the use of time-based features as a methodology to detect and classify DDoS traffic like they have been used to detect and classify Tor traffic, we will now create datasets containing only the time-based features. Lashkari et al. used a set of 23 time based features given by the pic below, but in addition to those 23, there are 2 more:\n",
    " * Forward Inter Arival Time Total (Fwd IAT Total)\n",
    " * Backward Inter Arrival Time Total (Bwd IAT Total)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Feature descriptions used by Lashkari et al, 2017 in their conference paper -- Characterization of Tor Traffic using Time based Features](./assets/CIC_feature_descriptions.png \"Feature descriptions used by Lashkari et al, 2017 in their conference paper -- Characterization of Tor Traffic using Time based Features\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# a list of all the time based features, as they are given in the dataframes we are dealing with. \r\n",
    "# We also add Label to make a total of 26 features\r\n",
    "time_based_features: list = [\r\n",
    "    'Fwd IAT Mean'  , 'Fwd IAT Std'    , 'Fwd IAT Max'   , 'Fwd IAT Min'  , \r\n",
    "    'Bwd IAT Mean'  , 'Bwd IAT Std'    , 'Bwd IAT Max'   , 'Bwd IAT Min'  , \r\n",
    "    'Flow IAT Mean' , 'Flow IAT Std'   , 'Flow IAT Max'  , 'Flow IAT Min' , \r\n",
    "    'Active Mean'   , 'Active Std'     , 'Active Max'    , 'Active Min'   , \r\n",
    "    'Idle Mean'     , 'Idle Std'       , 'Idle Max'      , 'Idle Min'     ,  \r\n",
    "    'Flow Bytes/s'  , 'Flow Packets/s' , 'Flow Duration' ,\r\n",
    "    'Fwd IAT Total' , 'Bwd IAT Total'  , 'Label'\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "Time_Based_Benign_vs_DDoS = Benign_vs_DDoS[time_based_features]\r\n",
    "\r\n",
    "Time_Based_Benign_vs_DDoS.to_csv(\"./prepared/timebased/Benign_vs_DDoS.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "Time_Based_Benign_Many_vs_Many = Benign_Many_vs_Many[time_based_features]\r\n",
    "\r\n",
    "Time_Based_Benign_Many_vs_Many.to_csv(\"./prepared/timebased/Benign_Many_vs_Many.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Time_Based_Benign_vs_DDoS.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(225459, 26)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for key in attack_samples.keys():\r\n",
    "    \r\n",
    "    total_attacks = attack_samples[key].shape[0]\r\n",
    "    total_each_other_type = int(total_attacks / 12)\r\n",
    "    \r\n",
    "    other_type_list = []\r\n",
    "    for attack in attack_samples.keys():\r\n",
    "        if attack != key:\r\n",
    "            other_type_list.append(attack_samples[attack].sample(n=total_each_other_type, random_state=seed))\r\n",
    "    \r\n",
    "    other_type_list.append(benign_df.sample(n=total_each_other_type, random_state=seed))\r\n",
    "\r\n",
    "    other_df = pd.concat(other_type_list, ignore_index=True)\r\n",
    "\r\n",
    "    to_replace = list(attack_samples.keys())\r\n",
    "    to_replace.append('BENIGN')\r\n",
    "\r\n",
    "    other_df.replace(to_replace, value=f'NOT{key}', inplace=True)\r\n",
    "\r\n",
    "    attack_df = attack_samples[key]\r\n",
    "\r\n",
    "    Attack_vs_all = pd.concat([attack_df, other_df], ignore_index=True)\r\n",
    "\r\n",
    "    Time_Based_Attack_vs_all = Attack_vs_all[time_based_features]\r\n",
    "\r\n",
    "    Time_Based_Attack_vs_all.to_csv(f'./prepared/timebased/{key}_vs_all.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "Time_Based_Attacks_Many_vs_Many = Attacks_Many_vs_Many[time_based_features]\r\n",
    "\r\n",
    "Time_Based_Attacks_Many_vs_Many.to_csv(\"./prepared/timebased/Attacks_Many_vs_Many.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# here we generate attack vs benign datasets. one for each attack type\r\n",
    "for key in attack_samples.keys():\r\n",
    "    \r\n",
    "    total_benign = benign_df.shape[0]\r\n",
    "\r\n",
    "    attack_df = attack_samples[key].sample(n=total_benign)\r\n",
    "\r\n",
    "    Attack_vs_Benign = pd.concat([attack_df, benign_df], ignore_index=True)\r\n",
    "\r\n",
    "    Time_Based_Attack_vs_Benign = Attack_vs_Benign[time_based_features]\r\n",
    "\r\n",
    "    Time_Based_Attack_vs_Benign.to_csv(f'./prepared/timebased/{key}_vs_benign.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# here we generate attack vs basket datasets. one for each attack type without benign samples\r\n",
    "for key in attack_samples.keys():\r\n",
    "    \r\n",
    "    total_attacks = attack_samples[key].shape[0]\r\n",
    "    total_each_other_type = int(total_attacks / 11)\r\n",
    "    \r\n",
    "    other_type_list = []\r\n",
    "    for attack in attack_samples.keys():\r\n",
    "        if attack != key:\r\n",
    "            other_type_list.append(attack_samples[attack].sample(n=total_each_other_type, random_state=seed))\r\n",
    "    \r\n",
    "\r\n",
    "    other_df = pd.concat(other_type_list, ignore_index=True)\r\n",
    "\r\n",
    "    to_replace = list(attack_samples.keys())\r\n",
    "    to_replace.append('BENIGN')\r\n",
    "\r\n",
    "    other_df.replace(to_replace=to_replace, value=f'DDOS', inplace=True)\r\n",
    "\r\n",
    "    attack_df = attack_samples[key]\r\n",
    "\r\n",
    "    Attack_vs_DDoS = pd.concat([attack_df, other_df], ignore_index=True)\r\n",
    "\r\n",
    "    Time_Based_Attack_vs_DDoS = Attack_vs_DDoS[time_based_features]\r\n",
    "\r\n",
    "    Time_Based_Attack_vs_DDoS.to_csv(f'./prepared/timebased/{key}_vs_ddos.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}