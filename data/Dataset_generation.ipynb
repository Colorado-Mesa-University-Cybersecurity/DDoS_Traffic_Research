{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generation\n",
    "\n",
    "Here we will generate 2 batches of datasets for our multi-class classification experiments. \n",
    "\n",
    "First we produce our baseline datasets containing most of the features present in CIC_DDoS2019, and then we produce our time-based feature datasets, each containing only the 25 time-based features as well as a label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing relavent libraries, setting a seed for reproducibility, and by printing out the versions of the libraries we are using for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    python:\t3.7.10\n",
      "\n",
      "    \tmatplotlib:\t3.3.4\n",
      "    \tnumpy:\t\t1.20.3\n",
      "    \tpandas:\t\t1.2.5\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import os, platform, pprint, sys\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "seed: int = 14\n",
    "\n",
    "# set up pretty printer for easier data evaluation\n",
    "pretty = pprint.PrettyPrinter(indent=4, width=30).pprint\n",
    "\n",
    "print(\n",
    "    f'''\n",
    "    python:\\t{platform.python_version()}\n",
    "\n",
    "    \\tmatplotlib:\\t{mpl.__version__}\n",
    "    \\tnumpy:\\t\\t{np.__version__}\n",
    "    \\tpandas:\\t\\t{pd.__version__}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Next, we do some preliminary set up. We list the data files we will be using and a list of new column names for the datasets that is more readable and understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_1: list = [\n",
    "    'DrDoS_DNS.csv'   , 'DrDoS_LDAP.csv'    ,\n",
    "    'DrDoS_MSSQL.csv' , 'DrDoS_NetBIOS.csv' ,\n",
    "    'DrDoS_NTP.csv'   , 'DrDoS_SNMP.csv'    ,\n",
    "    'DrDoS_SSDP.csv'  , 'DrDoS_UDP.csv'     ,\n",
    "    'Syn.csv'         , 'TFTP.csv'          ,\n",
    "    'UDPLag.csv'      ,    \n",
    "]\n",
    "    \n",
    "data_set_2: list = [\n",
    "    'LDAP.csv'    ,'MSSQL.csv'   ,\n",
    "    'NetBIOS.csv' ,'Portmap.csv' ,   \n",
    "    'Syn.csv'     ,'UDP.csv'     ,\n",
    "    'UDPLag.csv'  ,\n",
    "]\n",
    "\n",
    "data_set: list = data_set_1 + data_set_2\n",
    "\n",
    "\n",
    "# a list of DDoS attack types with indicies that map to the indicies of data_set\n",
    "data_location: list = [ \n",
    "    'DNS' , 'LDAP'  , 'MSSQL', 'NetBIOS', 'NTP'    , 'SNMP'   , 'SSDP', 'UDP', 'Syn'   , \n",
    "    'TFTP', 'UDPLag', 'LDAP' , 'MSSQL'  , 'NetBIOS', 'Portmap', 'Syn' , 'UDP', 'UDPLag',\n",
    "]\n",
    "\n",
    "\n",
    "# standardized column names for our data\n",
    "new_column_names: dict = {\n",
    "    'Unnamed: 0'                :'Unnamed'                  , 'Flow ID'                     :'Flow ID'                      ,\n",
    "    ' Source IP'                :'Source IP'                , ' Source Port'                :'Source Port'                  ,\n",
    "    ' Destination IP'           :'Destination IP'           , ' Destination Port'           :'Destination Port'             ,\n",
    "    ' Protocol'                 :'Protocol'                 , ' Total Length of Bwd Packets':'Total Length of Bwd Packets'  ,     \n",
    "    ' Flow Duration'            :'Flow Duration'            , ' Total Fwd Packets'          :'Total Fwd Packets'            , \n",
    "    ' Total Backward Packets'   :'Total Backward Packets'   , 'Total Length of Fwd Packets' :'Total Length of Fwd Packets'  ,\n",
    "    ' Timestamp'                :'Timestamp'                , ' Init_Win_bytes_backward'    :'Init Win bytes backward'      ,\n",
    "    ' Fwd Packet Length Max'    :'Fwd Packet Length Max'    , ' Fwd Packet Length Min'      :'Fwd Packet Length Min'        ,\n",
    "    ' Fwd Packet Length Mean'   :'Fwd Packet Length Mean'   , ' Fwd Packet Length Std'      :'Fwd Packet Length Std'        ,\n",
    "    'Bwd Packet Length Max'     :'Bwd Packet Length Max'    , ' Bwd Packet Length Min'      :'Bwd Packet Length Min'        ,\n",
    "    ' Bwd Packet Length Mean'   :'Bwd Packet Length Mean'   , ' Bwd Packet Length Std'      :'Bwd Packet Length Std'        ,\n",
    "    'Flow Bytes/s'              :'Flow Bytes/s'             , ' Flow Packets/s'             :'Flow Packets/s'               ,\n",
    "    ' Flow IAT Mean'            :'Flow IAT Mean'            , ' Flow IAT Std'               :'Flow IAT Std'                 ,\n",
    "    ' Flow IAT Max'             :'Flow IAT Max'             , ' Flow IAT Min'               :'Flow IAT Min'                 ,\n",
    "    'Fwd IAT Total'             :'Fwd IAT Total'            , ' Fwd IAT Mean'               :'Fwd IAT Mean'                 ,\n",
    "    ' Fwd IAT Std'              :'Fwd IAT Std'              , ' Fwd IAT Max'                :'Fwd IAT Max'                  ,\n",
    "    ' Fwd IAT Min'              :'Fwd IAT Min'              , 'Bwd IAT Total'               :'Bwd IAT Total'                ,    \n",
    "    ' Bwd IAT Mean'             :'Bwd IAT Mean'             , ' Bwd IAT Std'                :'Bwd IAT Std'                  ,\n",
    "    ' Bwd IAT Max'              :'Bwd IAT Max'              , ' Bwd IAT Min'                :'Bwd IAT Min'                  ,\n",
    "    'Fwd PSH Flags'             :'Fwd PSH Flags'            , ' Bwd PSH Flags'              :'Bwd PSH Flags'                , \n",
    "    ' Fwd URG Flags'            :'Fwd URG Flags'            , ' Bwd URG Flags'              :'Bwd URG Flags'                ,\n",
    "    ' Fwd Header Length'        :'Fwd Header Length'        , ' Bwd Header Length'          :'Bwd Header Length'            , \n",
    "    'Fwd Packets/s'             :'Fwd Packets/s'            , ' Bwd Packets/s'              :'Bwd Packets/s'                , \n",
    "    ' Min Packet Length'        :'Min Packet Length'        , ' Max Packet Length'          :'Max Packet Length'            , \n",
    "    ' Packet Length Mean'       :'Packet Length Mean'       , ' Packet Length Std'          :'Packet Length Std'            , \n",
    "    ' Packet Length Variance'   :'Packet Length Variance'   , 'FIN Flag Count'              :'FIN Flag Count'               ,\n",
    "    ' SYN Flag Count'           :'SYN Flag Count'           , ' RST Flag Count'             :'RST Flag Count'               ,\n",
    "    ' PSH Flag Count'           :'PSH Flag Count'           , ' ACK Flag Count'             :'ACK Flag Count'               , \n",
    "    ' URG Flag Count'           :'URG Flag Count'           , ' CWE Flag Count'             :'CWE Flag Count'               , \n",
    "    ' ECE Flag Count'           :'ECE Flag Count'           , ' Down/Up Ratio'              :'Down/Up Ratio'                ,\n",
    "    ' Average Packet Size'      :'Average Packet Size'      , ' Avg Fwd Segment Size'       :'Avg Fwd Segment Size'         ,\n",
    "    ' Avg Bwd Segment Size'     :'Avg Bwd Segment Size'     , ' Fwd Header Length.1'        :'Fwd Header Length.1'          , \n",
    "    'Fwd Avg Bytes/Bulk'        :'Fwd Avg Bytes/Bulk'       , ' Inbound'                    :'Inbound'                      , \n",
    "    ' Fwd Avg Packets/Bulk'     :'Fwd Avg Packets/Bulk'     , ' Fwd Avg Bulk Rate'          :'Fwd Avg Bulk Rate'            , \n",
    "    ' Bwd Avg Bytes/Bulk'       :'Bwd Avg Bytes/Bulk'       , ' Bwd Avg Packets/Bulk'       :'Bwd Avg Packets/Bulk'         ,\n",
    "    'Bwd Avg Bulk Rate'         :'Bwd Avg Bulk Rate'        , 'Subflow Fwd Packets'         :'Subflow Fwd Packets'          ,\n",
    "    ' Subflow Fwd Bytes'        :'Subflow Fwd Bytes'        , ' Subflow Bwd Packets'        :'Subflow Bwd Packets'          ,\n",
    "    ' Subflow Bwd Bytes'        :'Subflow Bwd Bytes'        , 'Init_Win_bytes_forward'      :'Init Win bytes forward'       ,\n",
    "    ' act_data_pkt_fwd'         :'act data pkt fwd'         , ' min_seg_size_forward'       :'min seg size forward'         ,     \n",
    "    'Active Mean'               :'Active Mean'              , ' Active Std'                 :'Active Std'                   ,\n",
    "    ' Active Max'               :'Active Max'               , ' Active Min'                 :'Active Min'                   , \n",
    "    'Idle Mean'                 :'Idle Mean'                , ' Idle Std'                   :'Idle Std'                     ,\n",
    "    ' Idle Max'                 :'Idle Max'                 , ' Idle Min'                   :'Idle Min'                     ,\n",
    "    'SimillarHTTP'              :'SimillarHTTP'             , ' Label'                      :'Label'                        ,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be cleaning 18 files:\n",
      "Benign samples will be grabbed from each dataset and saved separately\n",
      "\n",
      "[   './original/01-12/DrDoS_DNS.csv',\n",
      "    './original/01-12/DrDoS_LDAP.csv',\n",
      "    './original/01-12/DrDoS_MSSQL.csv',\n",
      "    './original/01-12/DrDoS_NetBIOS.csv',\n",
      "    './original/01-12/DrDoS_NTP.csv',\n",
      "    './original/01-12/DrDoS_SNMP.csv',\n",
      "    './original/01-12/DrDoS_SSDP.csv',\n",
      "    './original/01-12/DrDoS_UDP.csv',\n",
      "    './original/01-12/Syn.csv',\n",
      "    './original/01-12/TFTP.csv',\n",
      "    './original/01-12/UDPLag.csv',\n",
      "    './original/03-11/LDAP.csv',\n",
      "    './original/03-11/MSSQL.csv',\n",
      "    './original/03-11/NetBIOS.csv',\n",
      "    './original/03-11/Portmap.csv',\n",
      "    './original/03-11/Syn.csv',\n",
      "    './original/03-11/UDP.csv',\n",
      "    './original/03-11/UDPLag.csv']\n"
     ]
    }
   ],
   "source": [
    "def get_file_path(directory: str):\n",
    "    '''\n",
    "        Closure that will return a function that returns the filepath to the directory given to the closure\n",
    "    '''\n",
    "\n",
    "    def func(file: str) -> str:\n",
    "        return os.path.join(directory, file)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "# locations of the data files relative to current directory\n",
    "data_path_1: str = './original/01-12/'\n",
    "data_path_2: str = './original/03-11/'\n",
    "\n",
    "\n",
    "# use the get_file_path closure to create a function that will return the path to a file\n",
    "file_path_1 = get_file_path(data_path_1)\n",
    "file_path_2 = get_file_path(data_path_2)\n",
    "\n",
    "\n",
    "# a list of all complete filepaths relative to current directory with indicies mapped to the indicies of data_set\n",
    "file_set: list = list(map(file_path_1, data_set_1))\n",
    "file_set.extend(list(map(file_path_2, data_set_2)))\n",
    "\n",
    "\n",
    "print(f'We will be cleaning {len(file_set)} files:')\n",
    "print(f'Benign samples will be grabbed from each dataset and saved separately\\n')\n",
    "pretty(file_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our file paths, we set up a list of features to prune during our preprocessing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune is a list of all features we know we don't want to use\n",
    "# Unnamed is eliminated because it is un-labeled and we cannot verify what it qualities of the data if describes\n",
    "# Fwd Header Length.1 is eliminated because it is a duplicate\n",
    "# all the other features are eliminated because they are string values and cannot be used for classification\n",
    "prune: list = [\n",
    "    'Fwd Header Length.1',\n",
    "    'Unnamed',\n",
    "    'Source Port',\n",
    "    'Destination Port',\n",
    "    'Flow ID',\n",
    "    'Source IP',\n",
    "    'Destination IP',\n",
    "    'Timestamp',\n",
    "    'SimillarHTTP'\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maranhao et al. found in their study 'Tensor based framework for Distributed Denial of Service attack detection' that nine features were filled with only 0 values for every data collection in the dataset. Since an empty column of zeros will not contribute to the model's performance, we will remove those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be pruning 18 features\n",
      "\t1:\tFwd Header Length.1\n",
      "\t2:\tUnnamed\n",
      "\t3:\tSource Port\n",
      "\t4:\tDestination Port\n",
      "\t5:\tFlow ID\n",
      "\t6:\tSource IP\n",
      "\t7:\tDestination IP\n",
      "\t8:\tTimestamp\n",
      "\t9:\tSimillarHTTP\n",
      "\t10:\tFwd URG Flags\n",
      "\t11:\tBwd URG Flags\n",
      "\t12:\tFwd PSH Flags\n",
      "\t13:\tFwd Avg Bytes/Bulk\n",
      "\t14:\tFwd Avg Packets/Bulk\n",
      "\t15:\tFwd Avg Bulk Rate\n",
      "\t16:\tBwd Avg Bytes/Bulk\n",
      "\t17:\tBwd Avg Packets/Bulk\n",
      "\t18:\tBwd Avg Bulk Rate\n"
     ]
    }
   ],
   "source": [
    "# toPrune is a list of features with empty columns of 0s\n",
    "toPrune: list = [\n",
    "    'Fwd URG Flags',\n",
    "    'Bwd URG Flags',\n",
    "    'Fwd PSH Flags',\n",
    "    'Fwd Avg Bytes/Bulk',\n",
    "    'Fwd Avg Packets/Bulk',\n",
    "    'Fwd Avg Bulk Rate',\n",
    "    'Bwd Avg Bytes/Bulk',\n",
    "    'Bwd Avg Packets/Bulk',\n",
    "    'Bwd Avg Bulk Rate'\n",
    "]\n",
    "\n",
    "for i in toPrune:\n",
    "    if i not in prune:\n",
    "        prune.append(i)\n",
    "\n",
    "print(f'We will be pruning {len(prune)} features')\n",
    "for i, x in enumerate(prune):\n",
    "    print(f'\\t{i+1}:\\t{x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Data Cleaning\n",
    "\n",
    "Now that the preliminaries are done, we start processing the data. First we define some functions to load and clean the data, then we combine the data into dataframes based on their DDoS attack type. We keep the data manageable by sampling it down to sets of a million samples, using our seed to ensure that the results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will take a dataframe and remove the values from prune \n",
    "        Inf values will also be removed from Flow Bytes/s and Flow Packets/s\n",
    "        once appropriate rows and columns have been removed, we will return\n",
    "        the dataframe with the appropriate values\n",
    "    '''\n",
    "\n",
    "    # remove the features in the prune list    \n",
    "    for col in prune:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "            \n",
    "    \n",
    "    # drop missing values/NaN etc.\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    \n",
    "    # Search through dataframe for any Infinite or NaN values in various forms that were not picked up previously\n",
    "    invalid_values: list = [\n",
    "        np.inf, np.nan, 'Infinity', 'inf', 'NaN', 'nan'\n",
    "    ]\n",
    "    \n",
    "    for col in df.columns:\n",
    "        for value in invalid_values:\n",
    "            indexNames = df[df[col] == value].index\n",
    "            if not indexNames.empty:\n",
    "                print(f'deleting {len(indexNames)} rows with Infinity in column {col}')\n",
    "                df.drop(indexNames, inplace=True)\n",
    "\n",
    "\n",
    "    # Standardize the contents of the Label column\n",
    "    df = df.replace( ['DrDoS_DNS'], 'DNS')\n",
    "    df = df.replace( ['DrDoS_LDAP'], 'LDAP')\n",
    "    df = df.replace( ['DrDoS_MSSQL'], 'MSSQL')\n",
    "    df = df.replace( ['DrDoS_NetBIOS'], 'NetBIOS')\n",
    "    df = df.replace( ['DrDoS_NTP'], 'NTP')\n",
    "    df = df.replace( ['DrDoS_SNMP'], 'SNMP')\n",
    "    df = df.replace( ['DrDoS_SSDP'], 'SSDP')\n",
    "    df = df.replace( ['DrDoS_UDP'], 'UDP')\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(filePath: str) -> tuple:\n",
    "    '''\n",
    "        Loads the Dataset from the given filepath and caches it for quick access in the future\n",
    "        Function will only work when filepath is a .csv file\n",
    "        After the data is loaded, the benign samples are split and saved in a list\n",
    "        the malicious samples are split and saved in a dictionary of lists indexed by attack type\n",
    "        only the top million malicious samples are kept\n",
    "    '''\n",
    "\n",
    "    # slice off the ./CSV/ from the filePath\n",
    "    if filePath[0] == '.' and filePath[1] == '/':\n",
    "        filePathClean: str = filePath[11::]\n",
    "        pickleDump: str = f'./cache/{filePathClean}.pickle'\n",
    "    else:\n",
    "        pickleDump: str = f'./cache/{filePath}.pickle'\n",
    "    \n",
    "    print(f'Loading Dataset: {filePath}')\n",
    "    print(f'\\tTo Dataset Cache: {pickleDump}\\n')\n",
    "    \n",
    "    # check if data already exists within cache\n",
    "    if os.path.exists(pickleDump):\n",
    "        df = pd.read_pickle(pickleDump)\n",
    "        \n",
    "    # if not, load data and clean it before caching it\n",
    "    else:\n",
    "        df = pd.read_csv(filePath, low_memory=True)\n",
    "        df.to_pickle(pickleDump)\n",
    "\n",
    "    df = df.rename(columns=new_column_names)\n",
    "    \n",
    "    # split the data into benign and malicious samples, keeping only the top 1 milliion\n",
    "    # (+ 200 thousand to replace samples removed by cleaning) malicious samples\n",
    "    benignSamples = df[df['Label'] == 'BENIGN']\n",
    "    maliciousSamples = df[df['Label'] != 'BENIGN']\n",
    "    if maliciousSamples.shape[0] > 1200000:\n",
    "        maliciousSamples = maliciousSamples.sample(n=1200000, random_state=seed)\n",
    "\n",
    "    print(f'\\tLoaded {df.shape[0]} Samples as {benignSamples.shape[0]} Benign samples and {maliciousSamples.shape[0]} Malicious samples\\n')\n",
    "\n",
    "    return (benignSamples, maliciousSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset: ./original/01-12/DrDoS_DNS.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/DrDoS_DNS.csv.pickle\n",
      "\n",
      "\tLoaded 5074413 Samples as 3402 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 26 rows with Infinity in column Flow Bytes/s\n",
      "deleting 38356 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/DrDoS_LDAP.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/DrDoS_LDAP.csv.pickle\n",
      "\n",
      "\tLoaded 2181542 Samples as 1612 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 10 rows with Infinity in column Flow Bytes/s\n",
      "deleting 21382 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/DrDoS_MSSQL.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/DrDoS_MSSQL.csv.pickle\n",
      "\n",
      "\tLoaded 4524498 Samples as 2006 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 9 rows with Infinity in column Flow Bytes/s\n",
      "deleting 33524 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/DrDoS_NetBIOS.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/DrDoS_NetBIOS.csv.pickle\n",
      "\n",
      "\tLoaded 4094986 Samples as 1707 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 18 rows with Infinity in column Flow Bytes/s\n",
      "deleting 38082 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/DrDoS_NTP.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/DrDoS_NTP.csv.pickle\n",
      "\n",
      "\tLoaded 1217007 Samples as 14365 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 66 rows with Infinity in column Flow Bytes/s\n",
      "deleting 6935 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/DrDoS_SNMP.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/DrDoS_SNMP.csv.pickle\n",
      "\n",
      "\tLoaded 5161377 Samples as 1507 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 9 rows with Infinity in column Flow Bytes/s\n",
      "deleting 2514 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/DrDoS_SSDP.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/DrDoS_SSDP.csv.pickle\n",
      "\n",
      "\tLoaded 2611374 Samples as 763 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 7 rows with Infinity in column Flow Bytes/s\n",
      "deleting 19387 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/DrDoS_UDP.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/DrDoS_UDP.csv.pickle\n",
      "\n",
      "\tLoaded 3136802 Samples as 2157 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 24 rows with Infinity in column Flow Bytes/s\n",
      "deleting 15432 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/Syn.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/Syn.csv.pickle\n",
      "\n",
      "\tLoaded 1582681 Samples as 392 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 8 rows with Infinity in column Flow Bytes/s\n",
      "deleting 30 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/TFTP.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/TFTP.csv.pickle\n",
      "\n",
      "\tLoaded 20107827 Samples as 25247 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 127 rows with Infinity in column Flow Bytes/s\n",
      "deleting 33223 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/01-12/UDPLag.csv\n",
      "\tTo Dataset Cache: ./cache/01-12/UDPLag.csv.pickle\n",
      "\n",
      "\tLoaded 370605 Samples as 3705 Benign samples and 366900 Malicious samples\n",
      "\n",
      "deleting 21 rows with Infinity in column Flow Bytes/s\n",
      "deleting 250 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/03-11/LDAP.csv\n",
      "\tTo Dataset Cache: ./cache/03-11/LDAP.csv.pickle\n",
      "\n",
      "\tLoaded 2113234 Samples as 5124 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 58 rows with Infinity in column Flow Bytes/s\n",
      "deleting 30607 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/03-11/MSSQL.csv\n",
      "\tTo Dataset Cache: ./cache/03-11/MSSQL.csv.pickle\n",
      "\n",
      "\tLoaded 5775786 Samples as 2794 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 20 rows with Infinity in column Flow Bytes/s\n",
      "deleting 41891 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/03-11/NetBIOS.csv\n",
      "\tTo Dataset Cache: ./cache/03-11/NetBIOS.csv.pickle\n",
      "\n",
      "\tLoaded 3455899 Samples as 1321 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 21 rows with Infinity in column Flow Bytes/s\n",
      "deleting 45375 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/03-11/Portmap.csv\n",
      "\tTo Dataset Cache: ./cache/03-11/Portmap.csv.pickle\n",
      "\n",
      "\tLoaded 191694 Samples as 4734 Benign samples and 186960 Malicious samples\n",
      "\n",
      "deleting 36 rows with Infinity in column Flow Bytes/s\n",
      "deleting 9763 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/03-11/Syn.csv\n",
      "\tTo Dataset Cache: ./cache/03-11/Syn.csv.pickle\n",
      "\n",
      "\tLoaded 4320541 Samples as 35790 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 385 rows with Infinity in column Flow Bytes/s\n",
      "deleting 79056 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/03-11/UDP.csv\n",
      "\tTo Dataset Cache: ./cache/03-11/UDP.csv.pickle\n",
      "\n",
      "\tLoaded 3782206 Samples as 3134 Benign samples and 1200000 Malicious samples\n",
      "\n",
      "deleting 47 rows with Infinity in column Flow Bytes/s\n",
      "deleting 24642 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Loading Dataset: ./original/03-11/UDPLag.csv\n",
      "\tTo Dataset Cache: ./cache/03-11/UDPLag.csv.pickle\n",
      "\n",
      "\tLoaded 725165 Samples as 4068 Benign samples and 721097 Malicious samples\n",
      "\n",
      "deleting 50 rows with Infinity in column Flow Bytes/s\n",
      "deleting 50649 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "Benign Samples: 112731\n"
     ]
    }
   ],
   "source": [
    "# set up a dictionary to hold all the malicious samples\n",
    "malicious_dict: dict = {}\n",
    "for i in range(len(data_location)):\n",
    "    malicious_dict[data_location[i]] = []\n",
    "\n",
    "\n",
    "# load the data and save the samples in a dictionary or list for further processing\n",
    "for i in range(len(data_set)):\n",
    "    benignSamples, maliciousSamples = load_data(file_set[i])\n",
    "    benignSamples = clean_data(benignSamples)\n",
    "    maliciousSamples = clean_data(maliciousSamples)\n",
    "    print()\n",
    "\n",
    "    if i == 0:\n",
    "        benign_list: list = [benignSamples]\n",
    "    else:\n",
    "        benign_list.append(benignSamples)\n",
    "\n",
    "    malicious_dict[data_location[i]].append(maliciousSamples)\n",
    "\n",
    "\n",
    "# save the benign samples as a single dataframe\n",
    "benign_df: pd.DataFrame = pd.concat(benign_list, ignore_index=True)\n",
    "print(f'Benign Samples: {benign_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNS : (1161642, 70)\n",
      "LDAP : (1178617, 70)\n",
      "LDAP : (1169393, 70)\n",
      "MSSQL : (1166476, 70)\n",
      "MSSQL : (1158109, 70)\n",
      "NetBIOS : (1161918, 70)\n",
      "NetBIOS : (1154624, 70)\n",
      "NTP : (1193062, 70)\n",
      "SNMP : (1197484, 70)\n",
      "SSDP : (1180613, 70)\n",
      "UDP : (1184567, 70)\n",
      "UDP : (1175358, 70)\n",
      "Syn : (1046564, 70)\n",
      "Syn : (1120924, 70)\n",
      "TFTP : (1166188, 70)\n",
      "UDPLag : (330518, 70)\n",
      "UDPLag : (670447, 70)\n",
      "Portmap : (177197, 70)\n"
     ]
    }
   ],
   "source": [
    "for key in malicious_dict.keys():\n",
    "    for entry in malicious_dict[key]:\n",
    "        print(key, ':', entry.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign : (112731, 70)\n",
      "DNS : (1000000, 70)\n",
      "LDAP : (1000000, 70)\n",
      "MSSQL : (1000000, 70)\n",
      "NetBIOS : (1000000, 70)\n",
      "NTP : (1000000, 70)\n",
      "SNMP : (1000000, 70)\n",
      "SSDP : (1000000, 70)\n",
      "UDP : (1000000, 70)\n",
      "Syn : (1000000, 70)\n",
      "TFTP : (1000000, 70)\n",
      "UDPLag : (1000000, 70)\n",
      "Portmap : (177197, 70)\n"
     ]
    }
   ],
   "source": [
    "attack_samples: dict = {}\n",
    "for key in malicious_dict.keys():\n",
    "    new_df = pd.concat(malicious_dict[key], ignore_index=True)\n",
    "    if new_df.shape[0] > 1000000: \n",
    "        attack_samples[key] = new_df.sample(n=1000000, random_state=seed)\n",
    "    else:\n",
    "        attack_samples[key] = new_df\n",
    "    del new_df\n",
    "\n",
    "print('Benign', ':', benign_df.shape)\n",
    "for key in attack_samples.keys():\n",
    "    print(key, ':', attack_samples[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline dataset generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our data in 13 dataframes, we can begin to create our datasets. We will form 13 datasets for one-vs-all multi-class classification.\n",
    "\n",
    "\n",
    "The first dataset will be the Benign vs DDoS dataset. It will be a 50/50 split of the Benign vs DDoS samples. The DDoS samples will be equal parts of each DDoS attack type. Since we have around 112 thousand benign samples, we will use 112,000 benign samples and 112,000 samples of each DDoS attack type. \n",
    "\n",
    "\n",
    "Datasets 2-12 will each be one of the DDoS attack types (except Portmap since it has less samples) vs a basket of all the other DDoS attack types and the benign samples. Since each of the DDoS attack types has a million samples, each of the datasets will have a 2 million samples.\n",
    "\n",
    "\n",
    "Dataset 13 will be the Portmap vs. all dataset. Since we have around 177 thousand Portmap samples, we will use 177,000 Portmap samples and 177,000 samples of each of the other DDoS attack types and the benign samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we make the first dataset, benign vs DDoS. It will be a 50/50 split between \n",
    "# benign and DDoS samples where the DDoS samples are chosen equally from a pool of\n",
    "# all the DDoS attack types.\n",
    "\n",
    "total_benign = benign_df.shape[0]\n",
    "total_each_attack_type = int(total_benign/12)\n",
    "\n",
    "DDoS_list = []\n",
    "for key in attack_samples.keys():\n",
    "    DDoS_list.append(attack_samples[key].sample(n=total_each_attack_type, random_state=seed))\n",
    "\n",
    "ddos_df = pd.concat(DDoS_list, ignore_index=True)\n",
    "\n",
    "to_replace = attack_samples.keys()\n",
    "ddos_df.replace(to_replace=to_replace, value=\"DDOS\", inplace=True)\n",
    "\n",
    "\n",
    "Benign_vs_DDoS = pd.concat([benign_df, ddos_df], ignore_index=True)\n",
    "\n",
    "Benign_vs_DDoS.to_csv(\"./prepared/baseline/Benign_vs_DDoS.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in attack_samples.keys():\n",
    "    \n",
    "    total_attacks = attack_samples[key].shape[0]\n",
    "    total_each_other_type = int(total_attacks / 12)\n",
    "    \n",
    "    other_type_list = []\n",
    "    for attack in attack_samples.keys():\n",
    "        if attack != key:\n",
    "            other_type_list.append(attack_samples[attack].sample(n=total_each_other_type, random_state=seed))\n",
    "    \n",
    "    other_type_list.append(benign_df.sample(n=total_each_other_type, random_state=seed))\n",
    "\n",
    "    other_df = pd.concat(other_type_list, ignore_index=True)\n",
    "\n",
    "    to_replace = list(attack_samples.keys())\n",
    "    to_replace.append('BENIGN')\n",
    "\n",
    "    other_df.replace(to_replace, value=f'NOT{key}', inplace=True)\n",
    "\n",
    "    attack_df = attack_samples[key]\n",
    "\n",
    "    Attack_vs_all = pd.concat([attack_df, other_df], ignore_index=True)\n",
    "\n",
    "    Attack_vs_all.to_csv(f'./prepared/baseline/{key}_vs_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225459, 70)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Benign_vs_DDoS.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Based Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since one of our research directions is investigating the use of time-based features as a methodology to detect and classify DDoS traffic like they have been used to detect and classify Tor traffic, we will now create datasets containing only the time-based features. Lashkari et al. used a set of 23 time based features given by the pic below, but in addition to those 23, there are 2 more:\n",
    " * Forward Inter Arival Time Total (Fwd IAT Total)\n",
    " * Backward Inter Arrival Time Total (Bwd IAT Total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Feature descriptions used by Lashkari et al, 2017 in their conference paper -- Characterization of Tor Traffic using Time based Features](./assets/CIC_feature_descriptions.png \"Feature descriptions used by Lashkari et al, 2017 in their conference paper -- Characterization of Tor Traffic using Time based Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of all the time based features, as they are given in the dataframes we are dealing with. \n",
    "# We also add Label to make a total of 26 features\n",
    "time_based_features: list = [\n",
    "    'Fwd IAT Mean'  , 'Fwd IAT Std'    , 'Fwd IAT Max'   , 'Fwd IAT Min'  , \n",
    "    'Bwd IAT Mean'  , 'Bwd IAT Std'    , 'Bwd IAT Max'   , 'Bwd IAT Min'  , \n",
    "    'Flow IAT Mean' , 'Flow IAT Std'   , 'Flow IAT Max'  , 'Flow IAT Min' , \n",
    "    'Active Mean'   , 'Active Std'     , 'Active Max'    , 'Active Min'   , \n",
    "    'Idle Mean'     , 'Idle Std'       , 'Idle Max'      , 'Idle Min'     ,  \n",
    "    'Flow Bytes/s'  , 'Flow Packets/s' , 'Flow Duration' ,\n",
    "    'Fwd IAT Total' , 'Bwd IAT Total'  , 'Label'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Time_Based_Benign_vs_DDoS = Benign_vs_DDoS[time_based_features]\n",
    "\n",
    "Time_Based_Benign_vs_DDoS.to_csv(\"./prepared/timebased/Benign_vs_DDoS.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225459, 26)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Time_Based_Benign_vs_DDoS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in attack_samples.keys():\n",
    "    \n",
    "    total_attacks = attack_samples[key].shape[0]\n",
    "    total_each_other_type = int(total_attacks / 12)\n",
    "    \n",
    "    other_type_list = []\n",
    "    for attack in attack_samples.keys():\n",
    "        if attack != key:\n",
    "            other_type_list.append(attack_samples[attack].sample(n=total_each_other_type, random_state=seed))\n",
    "    \n",
    "    other_type_list.append(benign_df.sample(n=total_each_other_type, random_state=seed))\n",
    "\n",
    "    other_df = pd.concat(other_type_list, ignore_index=True)\n",
    "\n",
    "    to_replace = list(attack_samples.keys())\n",
    "    to_replace.append('BENIGN')\n",
    "\n",
    "    other_df.replace(to_replace, value=f'NOT{key}', inplace=True)\n",
    "\n",
    "    attack_df = attack_samples[key]\n",
    "\n",
    "    Attack_vs_all = pd.concat([attack_df, other_df], ignore_index=True)\n",
    "\n",
    "    Time_Based_Attack_vs_all = Attack_vs_all[time_based_features]\n",
    "\n",
    "    Time_Based_Attack_vs_all.to_csv(f'./prepared/timebased/{key}_vs_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
