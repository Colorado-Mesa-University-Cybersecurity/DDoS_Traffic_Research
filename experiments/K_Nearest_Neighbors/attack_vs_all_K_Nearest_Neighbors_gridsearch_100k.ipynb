{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os, platform, pprint, sys\r\n",
    "import fastai\r\n",
    "import matplotlib as mpl\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import sklearn\r\n",
    "import yellowbrick as yb\r\n",
    "\r\n",
    "from fastai.tabular.data import TabularDataLoaders, TabularPandas\r\n",
    "from fastai.tabular.all import FillMissing, Categorify, Normalize, tabular_learner, accuracy, ClassificationInterpretation, ShowGraphCallback, RandomSplitter, range_of\r\n",
    "\r\n",
    "from sklearn.base import BaseEstimator\r\n",
    "from sklearn.metrics import accuracy_score, classification_report\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "from yellowbrick.model_selection import CVScores, LearningCurve, ValidationCurve\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "seed: int = 14\r\n",
    "\r\n",
    "\r\n",
    "# set up pretty printer for easier data evaluation\r\n",
    "pretty = pprint.PrettyPrinter(indent=4, width=30).pprint\r\n",
    "\r\n",
    "\r\n",
    "# declare file paths for the data we will be working on\r\n",
    "data_path_1: str = '../data/prepared/baseline/'\r\n",
    "data_path_2: str = '../data/prepared/timebased/'\r\n",
    "modelPath  : str = './models'\r\n",
    "\r\n",
    "\r\n",
    "# list the names of the datasets we will be using\r\n",
    "attacks : list = [ 'DNS', 'LDAP', 'MSSQL', 'NetBIOS', 'NTP', 'Portmap', 'SNMP', 'SSDP', 'Syn', 'TFTP', 'UDP', 'UDPLag' ]\r\n",
    "datasets: list = [\r\n",
    "    \"DNS_vs_all.csv\" , \"LDAP_vs_all.csv\"    , \"MSSQL_vs_all.csv\" , \"NetBIOS_vs_all.csv\" ,\r\n",
    "    \"NTP_vs_all.csv\" , \"Portmap_vs_all.csv\" , \"SNMP_vs_all.csv\"  , \"SSDP_vs_all.csv\"    ,\r\n",
    "    \"Syn_vs_all.csv\" , \"TFTP_vs_all.csv\"    , \"UDP_vs_all.csv\"   , \"UDPLag_vs_all.csv\"  ,\r\n",
    "]\r\n",
    "\r\n",
    "\r\n",
    "# set up enumeration of experiment types\r\n",
    "Baseline : int = 0\r\n",
    "Timebased: int = 1\r\n",
    "\r\n",
    "\r\n",
    "# print library and python versions for reproducibility\r\n",
    "print(\r\n",
    "    f'''\r\n",
    "    python:\\t{platform.python_version()}\r\n",
    "\r\n",
    "    \\tfastai:\\t\\t{fastai.__version__}\r\n",
    "    \\tmatplotlib:\\t{mpl.__version__}\r\n",
    "    \\tnumpy:\\t\\t{np.__version__}\r\n",
    "    \\tpandas:\\t\\t{pd.__version__}\r\n",
    "    \\tsklearn:\\t{sklearn.__version__}\r\n",
    "    \\tyellowbrick:\\t{yb.__version__}\r\n",
    "    '''\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "    python:\t3.7.10\n",
      "\n",
      "    \tfaiss:\t\t1.7.0\n",
      "    \tfastai:\t\t2.4.1\n",
      "    \tmatplotlib:\t3.3.4\n",
      "    \tnumpy:\t\t1.20.3\n",
      "    \tpandas:\t\t1.2.5\n",
      "    \tsklearn:\t0.24.2\n",
      "    \tyellowbrick:\t1.3.post1\n",
      "    \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def get_file_path(directory: str):\r\n",
    "    '''\r\n",
    "        Closure that will return a function that returns the filepath to the directory given to the closure\r\n",
    "    '''\r\n",
    "\r\n",
    "    def func(file: str) -> str:\r\n",
    "        return os.path.join(directory, file)\r\n",
    "\r\n",
    "    return func\r\n",
    "\r\n",
    "\r\n",
    "# use the get_file_path closure to create a function that will return the path to a file\r\n",
    "baseline_path  = get_file_path(data_path_1)\r\n",
    "timebased_path = get_file_path(data_path_2)\r\n",
    "\r\n",
    "\r\n",
    "# create a list of the paths to all of the dataset files\r\n",
    "baseline_files : list = list(map(baseline_path , datasets))\r\n",
    "timebased_files: list = list(map(timebased_path, datasets))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def load_data(filePath: str) -> pd.DataFrame:\r\n",
    "    '''\r\n",
    "        Loads the Dataset from the given filepath and caches it for quick access in the future\r\n",
    "        Function will only work when filepath is a .csv file\r\n",
    "    '''\r\n",
    "\r\n",
    "    # slice off the ./CSV/ from the filePath\r\n",
    "    if filePath[0] == '.' and filePath[1] == '.':\r\n",
    "        filePathClean: str = filePath[17::]\r\n",
    "        pickleDump: str = f'../data/cache/{filePathClean}.pickle'\r\n",
    "    else:\r\n",
    "        pickleDump: str = f'../data/cache/{filePath}.pickle'\r\n",
    "    \r\n",
    "    print(f'Loading Dataset: {filePath}')\r\n",
    "    print(f'\\tTo Dataset Cache: {pickleDump}\\n')\r\n",
    "    \r\n",
    "    # check if data already exists within cache\r\n",
    "    if os.path.exists(pickleDump):\r\n",
    "        df = pd.read_pickle(pickleDump)\r\n",
    "        \r\n",
    "    # if not, load data and cache it\r\n",
    "    else:\r\n",
    "        df = pd.read_csv(filePath, low_memory=True)\r\n",
    "        df.to_pickle(pickleDump)\r\n",
    "\r\n",
    "    \r\n",
    "    return df\r\n",
    "\r\n",
    "\r\n",
    "def run_experiment(df: pd.DataFrame, name: str) -> tuple:\r\n",
    "    '''\r\n",
    "        Run binary classification using K-Nearest Neighbors\r\n",
    "        returns the 7-tuple with the following indicies:\r\n",
    "        results: tuple = (name, model, classes, X_train, y_train, X_test, y_test)\r\n",
    "    '''\r\n",
    "\r\n",
    "    df: pd.DataFrame = ndf.sample(n=100000, random_state=seed)\r\n",
    "\r\n",
    "\r\n",
    "    # First we split the features into the dependent variable and \r\n",
    "    # continous and categorical features\r\n",
    "    dep_var: str = 'Label'\r\n",
    "    if 'Protocol' in df.columns:\r\n",
    "        categorical_features: list = ['Protocol']\r\n",
    "    else:\r\n",
    "        categorical_features: list = []\r\n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([dep_var]))\r\n",
    "\r\n",
    "\r\n",
    "    # Next, we set up the feature engineering pipeline, namely filling missing values\r\n",
    "    # encoding categorical features, and normalizing the continuous features\r\n",
    "    # all within a pipeline to prevent the normalization from leaking details\r\n",
    "    # about the test sets through the normalized mapping of the training sets\r\n",
    "    procs = [FillMissing, Categorify, Normalize]\r\n",
    "    splits = RandomSplitter(valid_pct=0.2, seed=seed)(range_of(df))\r\n",
    "    \r\n",
    "    \r\n",
    "    # The dataframe is loaded into a fastai datastructure now that \r\n",
    "    # the feature engineering pipeline has been set up\r\n",
    "    to = TabularPandas(\r\n",
    "        df            , y_names=dep_var                , \r\n",
    "        splits=splits , cat_names=categorical_features ,\r\n",
    "        procs=procs   , cont_names=continuous_features , \r\n",
    "    )\r\n",
    "\r\n",
    "\r\n",
    "    # We use fastai to quickly extract the names of the classes as they are mapped to the encodings\r\n",
    "    dls = to.dataloaders(bs=64)\r\n",
    "    mds = tabular_learner(dls)\r\n",
    "    classes : list = list(mds.dls.vocab)\r\n",
    "\r\n",
    "\r\n",
    "    # We extract the training and test datasets from the dataframe\r\n",
    "    X_train = to.train.xs.reset_index(drop=True)\r\n",
    "    X_test = to.valid.xs.reset_index(drop=True)\r\n",
    "    y_train = to.train.ys.values.ravel()\r\n",
    "    y_test = to.valid.ys.values.ravel()\r\n",
    "\r\n",
    "\r\n",
    "    # Now that we have the train and test datasets, we set up a gridsearch of the K-NN classifier\r\n",
    "    # using SciKitLearn and print the results \r\n",
    "    params = {\"n_neighbors\": range(1, 50)}\r\n",
    "    model = GridSearchCV(KNeighborsClassifier(), params)\r\n",
    "    model.fit(X_train, y_train)\r\n",
    "    prediction = model.predict(X_test)\r\n",
    "    report = classification_report(y_test, prediction)\r\n",
    "    print(report)\r\n",
    "    print(\"Best Parameters found by gridsearch:\")\r\n",
    "    print(model.best_params_)\r\n",
    "\r\n",
    "\r\n",
    "   # we add a target_type_ attribute to our model so yellowbrick knows how to make the visualizations\r\n",
    "    if len(classes) == 2:\r\n",
    "        model.target_type_ = 'binary'\r\n",
    "        # wrapped_model.target_type_ = 'binary'\r\n",
    "    elif len(classes) > 2:  \r\n",
    "        model.target_type_ = 'multiclass'\r\n",
    "        # wrapped_model.target_type_ = 'multiclass'\r\n",
    "    else:\r\n",
    "        print('Must be more than one class to perform classification')\r\n",
    "        raise ValueError('Wrong number of classes')\r\n",
    "\r\n",
    "\r\n",
    "    # Now that the classifier has been created and trained, we pass out our training values\r\n",
    "    # so that yellowbrick can use them to create various visualizations\r\n",
    "    return (name, model, classes, X_train, y_train, X_test, y_test)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "baseline_dfs : map = map( load_data    , baseline_files  )\r\n",
    "timebased_dfs: map = map( load_data    , timebased_files )\r\n",
    "experiments  : zip = zip( baseline_dfs , timebased_dfs   , attacks )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def experiment_runner():\r\n",
    "    '''\r\n",
    "        A generator that handles running the experiments\r\n",
    "    '''\r\n",
    "    num = 1\r\n",
    "    for baseline, timebased, info in experiments:\r\n",
    "        print(f'Running experiment #{num}:\\t{info}')\r\n",
    "\r\n",
    "        print('Baseline results')\r\n",
    "        baseline_results = run_experiment(baseline, f'{info}_vs_all_baseline')\r\n",
    "        \r\n",
    "        print('\\nTime-based results')\r\n",
    "        timebased_results = run_experiment(timebased, f'{info}_vs_all_timebased')\r\n",
    "        \r\n",
    "        num += 1\r\n",
    "        yield (baseline_results, timebased_results, info, num)\r\n",
    "\r\n",
    "\r\n",
    "def do_experiment(num: int) -> tuple:\r\n",
    "    '''\r\n",
    "        A function that runs the specific experiment specified\r\n",
    "    '''\r\n",
    "    index = num - 1\r\n",
    "    baseline = load_data(baseline_files[index])\r\n",
    "    timebased = load_data(timebased_files[index])\r\n",
    "    info = attacks[index]\r\n",
    "\r\n",
    "    print(f'Running experiment #{num}:\\t{info}')\r\n",
    "\r\n",
    "    print('Baseline results')\r\n",
    "    baseline_results = run_experiment(baseline, f'{info}_vs_all_baseline')\r\n",
    "    \r\n",
    "    print('\\nTime-based results')\r\n",
    "    timebased_results = run_experiment(timebased, f'{info}_vs_all_timebased')\r\n",
    "\r\n",
    "    return (baseline_results, timebased_results, info, num)\r\n",
    "\r\n",
    "\r\n",
    "experiment = experiment_runner()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #1: DNS vs All"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Dataset: ../data/prepared/baseline/DNS_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/baseline/DNS_vs_all.csv.pickle\n",
      "\n",
      "Loading Dataset: ../data/prepared/timebased/DNS_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/timebased/DNS_vs_all.csv.pickle\n",
      "\n",
      "Running experiment #1:\tDNS\n",
      "Baseline results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91      9955\n",
      "           1       0.97      0.84      0.90     10045\n",
      "\n",
      "    accuracy                           0.91     20000\n",
      "   macro avg       0.91      0.91      0.90     20000\n",
      "weighted avg       0.91      0.91      0.90     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 4}\n",
      "\n",
      "Time-based results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91     10005\n",
      "           1       0.97      0.84      0.90      9995\n",
      "\n",
      "    accuracy                           0.91     20000\n",
      "   macro avg       0.91      0.91      0.91     20000\n",
      "weighted avg       0.91      0.91      0.91     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 8}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #2: LDAP vs All"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Dataset: ../data/prepared/baseline/LDAP_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/baseline/LDAP_vs_all.csv.pickle\n",
      "\n",
      "Loading Dataset: ../data/prepared/timebased/LDAP_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/timebased/LDAP_vs_all.csv.pickle\n",
      "\n",
      "Running experiment #2:\tLDAP\n",
      "Baseline results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     10018\n",
      "           1       0.96      0.87      0.91      9982\n",
      "\n",
      "    accuracy                           0.92     20000\n",
      "   macro avg       0.92      0.91      0.91     20000\n",
      "weighted avg       0.92      0.92      0.91     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 13}\n",
      "\n",
      "Time-based results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.93     10041\n",
      "           1       0.99      0.87      0.93      9959\n",
      "\n",
      "    accuracy                           0.93     20000\n",
      "   macro avg       0.94      0.93      0.93     20000\n",
      "weighted avg       0.94      0.93      0.93     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 4}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #3: MSSQL vs All"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Dataset: ../data/prepared/baseline/MSSQL_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/baseline/MSSQL_vs_all.csv.pickle\n",
      "\n",
      "Loading Dataset: ../data/prepared/timebased/MSSQL_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/timebased/MSSQL_vs_all.csv.pickle\n",
      "\n",
      "Running experiment #3:\tMSSQL\n",
      "Baseline results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98     10134\n",
      "           1       0.97      0.99      0.98      9866\n",
      "\n",
      "    accuracy                           0.98     20000\n",
      "   macro avg       0.98      0.98      0.98     20000\n",
      "weighted avg       0.98      0.98      0.98     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 7}\n",
      "\n",
      "Time-based results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      9995\n",
      "           1       0.96      0.99      0.97     10005\n",
      "\n",
      "    accuracy                           0.97     20000\n",
      "   macro avg       0.97      0.97      0.97     20000\n",
      "weighted avg       0.97      0.97      0.97     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 5}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #4: NetBIOS vs All"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Dataset: ../data/prepared/baseline/NetBIOS_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/baseline/NetBIOS_vs_all.csv.pickle\n",
      "\n",
      "Loading Dataset: ../data/prepared/timebased/NetBIOS_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/timebased/NetBIOS_vs_all.csv.pickle\n",
      "\n",
      "Running experiment #4:\tNetBIOS\n",
      "Baseline results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.91      0.95      9958\n",
      "           1       0.92      0.99      0.95     10042\n",
      "\n",
      "    accuracy                           0.95     20000\n",
      "   macro avg       0.95      0.95      0.95     20000\n",
      "weighted avg       0.95      0.95      0.95     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 8}\n",
      "\n",
      "Time-based results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.95      9997\n",
      "           1       0.91      0.99      0.95     10003\n",
      "\n",
      "    accuracy                           0.95     20000\n",
      "   macro avg       0.95      0.95      0.95     20000\n",
      "weighted avg       0.95      0.95      0.95     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 6}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #5: NTP vs All"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Dataset: ../data/prepared/baseline/NTP_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/baseline/NTP_vs_all.csv.pickle\n",
      "\n",
      "Loading Dataset: ../data/prepared/timebased/NTP_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/timebased/NTP_vs_all.csv.pickle\n",
      "\n",
      "Running experiment #5:\tNTP\n",
      "Baseline results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     10012\n",
      "           1       1.00      1.00      1.00      9988\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 1}\n",
      "\n",
      "Time-based results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     10071\n",
      "           1       0.99      1.00      0.99      9929\n",
      "\n",
      "    accuracy                           0.99     20000\n",
      "   macro avg       0.99      0.99      0.99     20000\n",
      "weighted avg       0.99      0.99      0.99     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 1}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #6: Portmap vs All"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Dataset: ../data/prepared/baseline/Portmap_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/baseline/Portmap_vs_all.csv.pickle\n",
      "\n",
      "Loading Dataset: ../data/prepared/timebased/Portmap_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/timebased/Portmap_vs_all.csv.pickle\n",
      "\n",
      "Running experiment #6:\tPortmap\n",
      "Baseline results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96     10034\n",
      "           1       0.93      0.99      0.96      9966\n",
      "\n",
      "    accuracy                           0.96     20000\n",
      "   macro avg       0.96      0.96      0.96     20000\n",
      "weighted avg       0.96      0.96      0.96     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 7}\n",
      "\n",
      "Time-based results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95     10035\n",
      "           1       0.92      0.99      0.96      9965\n",
      "\n",
      "    accuracy                           0.96     20000\n",
      "   macro avg       0.96      0.96      0.96     20000\n",
      "weighted avg       0.96      0.96      0.96     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 6}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #7: SNMP vs All"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Dataset: ../data/prepared/baseline/SNMP_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/baseline/SNMP_vs_all.csv.pickle\n",
      "\n",
      "Loading Dataset: ../data/prepared/timebased/SNMP_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/timebased/SNMP_vs_all.csv.pickle\n",
      "\n",
      "Running experiment #7:\tSNMP\n",
      "Baseline results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92      9992\n",
      "           1       0.94      0.88      0.91     10008\n",
      "\n",
      "    accuracy                           0.91     20000\n",
      "   macro avg       0.92      0.91      0.91     20000\n",
      "weighted avg       0.92      0.91      0.91     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 24}\n",
      "\n",
      "Time-based results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     10064\n",
      "           1       0.95      0.87      0.91      9936\n",
      "\n",
      "    accuracy                           0.91     20000\n",
      "   macro avg       0.92      0.91      0.91     20000\n",
      "weighted avg       0.92      0.91      0.91     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 46}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #8: SSDP vs All"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Dataset: ../data/prepared/baseline/SSDP_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/baseline/SSDP_vs_all.csv.pickle\n",
      "\n",
      "Loading Dataset: ../data/prepared/timebased/SSDP_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/timebased/SSDP_vs_all.csv.pickle\n",
      "\n",
      "Running experiment #8:\tSSDP\n",
      "Baseline results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94     10056\n",
      "           1       0.91      0.98      0.94      9944\n",
      "\n",
      "    accuracy                           0.94     20000\n",
      "   macro avg       0.94      0.94      0.94     20000\n",
      "weighted avg       0.94      0.94      0.94     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 14}\n",
      "\n",
      "Time-based results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.93      9916\n",
      "           1       0.90      0.97      0.94     10084\n",
      "\n",
      "    accuracy                           0.93     20000\n",
      "   macro avg       0.94      0.93      0.93     20000\n",
      "weighted avg       0.94      0.93      0.93     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 11}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #9: Syn vs All"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Dataset: ../data/prepared/baseline/Syn_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/baseline/Syn_vs_all.csv.pickle\n",
      "\n",
      "Loading Dataset: ../data/prepared/timebased/Syn_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/timebased/Syn_vs_all.csv.pickle\n",
      "\n",
      "Running experiment #9:\tSyn\n",
      "Baseline results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97      9900\n",
      "           1       0.94      0.99      0.97     10100\n",
      "\n",
      "    accuracy                           0.97     20000\n",
      "   macro avg       0.97      0.97      0.97     20000\n",
      "weighted avg       0.97      0.97      0.97     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 14}\n",
      "\n",
      "Time-based results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96     10028\n",
      "           1       0.94      0.99      0.96      9972\n",
      "\n",
      "    accuracy                           0.96     20000\n",
      "   macro avg       0.96      0.96      0.96     20000\n",
      "weighted avg       0.96      0.96      0.96     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 7}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #10: TFTP vs All"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Dataset: ../data/prepared/baseline/TFTP_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/baseline/TFTP_vs_all.csv.pickle\n",
      "\n",
      "Loading Dataset: ../data/prepared/timebased/TFTP_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/timebased/TFTP_vs_all.csv.pickle\n",
      "\n",
      "Running experiment #10:\tTFTP\n",
      "Baseline results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      9968\n",
      "           1       1.00      0.99      1.00     10032\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 2}\n",
      "\n",
      "Time-based results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     10144\n",
      "           1       1.00      0.99      0.99      9856\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      0.99      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 2}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #11: UDP vs All"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Dataset: ../data/prepared/baseline/UDP_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/baseline/UDP_vs_all.csv.pickle\n",
      "\n",
      "Loading Dataset: ../data/prepared/timebased/UDP_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/timebased/UDP_vs_all.csv.pickle\n",
      "\n",
      "Running experiment #11:\tUDP\n",
      "Baseline results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.91      0.95      9868\n",
      "           1       0.92      0.99      0.95     10132\n",
      "\n",
      "    accuracy                           0.95     20000\n",
      "   macro avg       0.95      0.95      0.95     20000\n",
      "weighted avg       0.95      0.95      0.95     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 16}\n",
      "\n",
      "Time-based results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94     10032\n",
      "           1       0.90      0.98      0.94      9968\n",
      "\n",
      "    accuracy                           0.94     20000\n",
      "   macro avg       0.94      0.94      0.94     20000\n",
      "weighted avg       0.94      0.94      0.94     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 7}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #12: UDP-lag vs All"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Dataset: ../data/prepared/baseline/UDPLag_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/baseline/UDPLag_vs_all.csv.pickle\n",
      "\n",
      "Loading Dataset: ../data/prepared/timebased/UDPLag_vs_all.csv\n",
      "\tTo Dataset Cache: ../data/cache/timebased/UDPLag_vs_all.csv.pickle\n",
      "\n",
      "Running experiment #12:\tUDPLag\n",
      "Baseline results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92     10090\n",
      "           1       0.92      0.93      0.92      9910\n",
      "\n",
      "    accuracy                           0.92     20000\n",
      "   macro avg       0.92      0.92      0.92     20000\n",
      "weighted avg       0.92      0.92      0.92     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 12}\n",
      "\n",
      "Time-based results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91     10088\n",
      "           1       0.92      0.88      0.90      9912\n",
      "\n",
      "    accuracy                           0.90     20000\n",
      "   macro avg       0.90      0.90      0.90     20000\n",
      "weighted avg       0.90      0.90      0.90     20000\n",
      "\n",
      "Best Parameters found by gridsearch:\n",
      "{'n_neighbors': 6}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}