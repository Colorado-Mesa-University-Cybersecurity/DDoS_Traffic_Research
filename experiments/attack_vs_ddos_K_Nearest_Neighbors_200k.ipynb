{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Neural Network Experiments: Attack vs All"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we construct Deep Neural Networks using the fastai library. We then train the networks on the attack vs all datasets. These datasets are constructed by with a specific DDoS attack type and a basket of all other DDoS attack types as well as Benign samples. There are two collections of datasets:\r\n",
    " * Baseline: A dataset containing 70 features from the original 88 features used for baseline comparison.\r\n",
    " * Time-based: A dataset containing 26 features from the original 88 features, 23 of which are the time-based features used by Lashkari et al. in [this paper](https://www.researchgate.net/publication/314521450_Characterization_of_Tor_Traffic_using_Time_based_Features), 2 are time-based features ignored by Laskari et al. This dataset is used to evaluate the ability of a DNN to use time-based features to distingush between DDoS attacks.\r\n",
    "\r\n",
    "The goal of these experiments is to understand how effective a Deep Neural Network is at discriminating between different attack types as well as to see how well time-based features can be used to classify DDoS attacks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by importing relavent libraries, setting a seed for reproducibility, and by printing out the versions of the libraries we are using for reproducibility."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os, platform, pprint, sys\r\n",
    "import fastai\r\n",
    "import matplotlib as mpl\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import sklearn\r\n",
    "import yellowbrick as yb\r\n",
    "\r\n",
    "from fastai.tabular.data import TabularDataLoaders, TabularPandas\r\n",
    "from fastai.tabular.all import FillMissing, Categorify, Normalize, tabular_learner, accuracy, ClassificationInterpretation, ShowGraphCallback, RandomSplitter, range_of, MixedPrecision, FP16TestCallback\r\n",
    "\r\n",
    "from sklearn.base import BaseEstimator\r\n",
    "from sklearn.metrics import accuracy_score, classification_report\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "from yellowbrick.model_selection import CVScores, LearningCurve, ValidationCurve\r\n",
    "\r\n",
    "\r\n",
    "seed: int = 14\r\n",
    "\r\n",
    "\r\n",
    "# set up pretty printer for easier data evaluation\r\n",
    "pretty = pprint.PrettyPrinter(indent=4, width=30).pprint\r\n",
    "\r\n",
    "\r\n",
    "# declare file paths for the data we will be working on\r\n",
    "data_path_1: str = '../data/downsampled/baseline/'\r\n",
    "data_path_2: str = '../data/downsampled/timebased/'\r\n",
    "modelPath  : str = './models'\r\n",
    "\r\n",
    "\r\n",
    "# list the names of the datasets we will be using\r\n",
    "attacks : list = [ 'DNS', 'LDAP', 'MSSQL', 'NetBIOS', 'NTP', 'Portmap', 'SNMP', 'SSDP', 'Syn', 'TFTP', 'UDP', 'UDPLag' ]\r\n",
    "datasets: list = [\r\n",
    "    \"DNS_vs_ddos_200000.csv\" , \"LDAP_vs_ddos_200000.csv\"    , \"MSSQL_vs_ddos_200000.csv\" , \"NetBIOS_vs_ddos_200000.csv\" ,\r\n",
    "    \"NTP_vs_ddos_200000.csv\" , \"Portmap_vs_ddos_200000.csv\" , \"SNMP_vs_ddos_200000.csv\"  , \"SSDP_vs_ddos_200000.csv\"    ,\r\n",
    "    \"Syn_vs_ddos_200000.csv\" , \"TFTP_vs_ddos_200000.csv\"    , \"UDP_vs_ddos_200000.csv\"   , \"UDPLag_vs_ddos_200000.csv\"  ,\r\n",
    "]\r\n",
    "\r\n",
    "\r\n",
    "# enumerate dataset types\r\n",
    "Baseline : int = 0\r\n",
    "Timebased: int = 1\r\n",
    "\r\n",
    "\r\n",
    "# print library and python versions for reproducibility\r\n",
    "print(\r\n",
    "    f'''\r\n",
    "    python:\\t{platform.python_version()}\r\n",
    "\r\n",
    "    \\tfastai:\\t\\t{fastai.__version__}\r\n",
    "    \\tmatplotlib:\\t{mpl.__version__}\r\n",
    "    \\tnumpy:\\t\\t{np.__version__}\r\n",
    "    \\tpandas:\\t\\t{pd.__version__}\r\n",
    "    \\tsklearn:\\t{sklearn.__version__}\r\n",
    "    \\tyellowbrick:\\t{yb.__version__}\r\n",
    "    '''\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "    python:\t3.7.10\n",
      "\n",
      "    \tfastai:\t\t2.4.1\n",
      "    \tmatplotlib:\t3.3.4\n",
      "    \tnumpy:\t\t1.20.3\n",
      "    \tpandas:\t\t1.2.5\n",
      "    \tsklearn:\t0.24.2\n",
      "    \tyellowbrick:\t1.3.post1\n",
      "    \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If any of the versions of the libraries listed above are different on your system, the experiments may not be reproducible. The experiments may not even run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preliminaries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we create functions/classes to load the data, run the experiments, visualize the data, and various helper functions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def get_file_path(directory: str):\r\n",
    "    '''\r\n",
    "        Closure that will return a function that returns the filepath to the directory given to the closure\r\n",
    "    '''\r\n",
    "\r\n",
    "    def func(file: str) -> str:\r\n",
    "        return os.path.join(directory, file)\r\n",
    "\r\n",
    "    return func\r\n",
    "\r\n",
    "\r\n",
    "def load_data(filePath: str) -> pd.DataFrame:\r\n",
    "    '''\r\n",
    "        Loads the Dataset from the given filepath and caches it for quick access in the future\r\n",
    "        Function will only work when filepath is a .csv file\r\n",
    "    '''\r\n",
    "\r\n",
    "    # slice off the ./CSV/ from the filePath\r\n",
    "    if filePath[0] == '.' and filePath[1] == '.':\r\n",
    "        filePathClean: str = filePath[20::]\r\n",
    "        pickleDump: str = f'../data/cache/{filePathClean}.pickle'\r\n",
    "    else:\r\n",
    "        pickleDump: str = f'../data/cache/{filePath}.pickle'\r\n",
    "    \r\n",
    "    print(f'Loading Dataset: {filePath}')\r\n",
    "    print(f'\\tTo Dataset Cache: {pickleDump}\\n')\r\n",
    "    \r\n",
    "    # check if data already exists within cache\r\n",
    "    # if not, load data and cache it\r\n",
    "    if os.path.exists(pickleDump):\r\n",
    "        df = pd.read_pickle(pickleDump)\r\n",
    "    else:\r\n",
    "        df = pd.read_csv(filePath, low_memory=True)\r\n",
    "        df.to_pickle(pickleDump)\r\n",
    "    \r\n",
    "    return df\r\n",
    "\r\n",
    "\r\n",
    "class SklearnWrapper(BaseEstimator):\r\n",
    "    '''\r\n",
    "        A wrapper for fastai learners for creating visualizations using yellowbrick\r\n",
    "        code sourced from: \r\n",
    "        forums.fast.ai/t/fastai-with-yellowbrics-how-to-get-roc-curves-more/79408\r\n",
    "    '''\r\n",
    "    _estimator_type = \"classifier\"\r\n",
    "        \r\n",
    "    def __init__(self, model):\r\n",
    "        self.model = model\r\n",
    "        self.classes_ = list(self.model.dls.y.unique())\r\n",
    "    \r\n",
    "    def fit(self, X, y):\r\n",
    "        pass\r\n",
    "        \r\n",
    "    def score(self, X, y):\r\n",
    "        return accuracy_score(y, self.predict(X))\r\n",
    "    \r\n",
    "    def get_new_preds(self, X):\r\n",
    "        new_to = self.model.dls.valid_ds.new(X)\r\n",
    "        new_to.conts = new_to.conts.astype(np.float32)\r\n",
    "        new_dl = self.model.dls.valid.new(new_to)\r\n",
    "        with self.model.no_bar():\r\n",
    "            preds,_,dec_preds = self.model.get_preds(dl=new_dl, with_decoded=True)\r\n",
    "        return (preds, dec_preds)\r\n",
    "\r\n",
    "    def predict_proba(self, X):\r\n",
    "        return self.get_new_preds(X)[0].numpy()\r\n",
    "    \r\n",
    "    def predict(self, X):\r\n",
    "        return self.get_new_preds(X)[1].numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use run_experiment to execute each experiment which returns a 7-tuple containing the results of the experiment. \r\n",
    "\r\n",
    "We then define a series of visualization functions that take that 7-tuple and plot various graph, characterizing the results of the experiment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\r\n",
    "def run_experiment(df: pd.DataFrame, name: str) -> tuple:\r\n",
    "    '''\r\n",
    "        Run binary classification using K-Nearest Neighbors\r\n",
    "        returns the 7-tuple with the following indicies:\r\n",
    "        results: tuple = (name, model, classes, X_train, y_train, X_test, y_test)\r\n",
    "    '''\r\n",
    "\r\n",
    "    # First we split the features into the dependent variable and \r\n",
    "    # continous and categorical features\r\n",
    "    dep_var: str = 'Label'\r\n",
    "    if 'Protocol' in df.columns:\r\n",
    "        categorical_features: list = ['Protocol']\r\n",
    "    else:\r\n",
    "        categorical_features: list = []\r\n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([dep_var]))\r\n",
    "\r\n",
    "\r\n",
    "    # Next, we set up the feature engineering pipeline, namely filling missing values\r\n",
    "    # encoding categorical features, and normalizing the continuous features\r\n",
    "    # all within a pipeline to prevent the normalization from leaking details\r\n",
    "    # about the test sets through the normalized mapping of the training sets\r\n",
    "    procs = [FillMissing, Categorify, Normalize]\r\n",
    "    splits = RandomSplitter(valid_pct=0.2, seed=seed)(range_of(df))\r\n",
    "    \r\n",
    "    \r\n",
    "    # The dataframe is loaded into a fastai datastructure now that \r\n",
    "    # the feature engineering pipeline has been set up\r\n",
    "    to = TabularPandas(\r\n",
    "        df            , y_names=dep_var                , \r\n",
    "        splits=splits , cat_names=categorical_features ,\r\n",
    "        procs=procs   , cont_names=continuous_features , \r\n",
    "    )\r\n",
    "\r\n",
    "\r\n",
    "    # We use fastai to quickly extract the names of the classes as they are mapped to the encodings\r\n",
    "    dls = to.dataloaders(bs=64)\r\n",
    "    mds = tabular_learner(dls)\r\n",
    "    classes : list = list(mds.dls.vocab)\r\n",
    "\r\n",
    "\r\n",
    "    # We extract the training and test datasets from the dataframe\r\n",
    "    X_train = to.train.xs.reset_index(drop=True)\r\n",
    "    X_test = to.valid.xs.reset_index(drop=True)\r\n",
    "    y_train = to.train.ys.values.ravel()\r\n",
    "    y_test = to.valid.ys.values.ravel()\r\n",
    "\r\n",
    "\r\n",
    "    # Now that we have the train and test datasets, we set up a K-NN classifier\r\n",
    "    # using SciKitLearn and print the results \r\n",
    "    model = KNeighborsClassifier()\r\n",
    "    model.fit(X_train, y_train)\r\n",
    "    prediction = model.predict(X_test)\r\n",
    "    print(f'\\tAccuracy: {accuracy_score(y_test, prediction)}\\n')\r\n",
    "    report = classification_report(y_test, prediction)\r\n",
    "    print(report)\r\n",
    "\r\n",
    "\r\n",
    "   # we add a target_type_ attribute to our model so yellowbrick knows how to make the visualizations\r\n",
    "    if len(classes) == 2:\r\n",
    "        model.target_type_ = 'binary'\r\n",
    "    elif len(classes) > 2:  \r\n",
    "        model.target_type_ = 'multiclass'\r\n",
    "    else:\r\n",
    "        print('Must be more than one class to perform classification')\r\n",
    "        raise ValueError('Wrong number of classes')\r\n",
    "\r\n",
    "\r\n",
    "    # Now that the classifier has been created and trained, we pass out our training values\r\n",
    "    # so that yellowbrick can use them to create various visualizations\r\n",
    "    results: tuple = (name, model, classes, X_train, y_train, X_test, y_test)\r\n",
    "\r\n",
    "    return results\r\n",
    "\r\n",
    "\r\n",
    "def visualize_learning_curve_train(results: tuple) -> None:\r\n",
    "    '''\r\n",
    "        Takes a 7-tuple from the run_experiments function and creates a learning curve\r\n",
    "\r\n",
    "        results: tuple = (name, model, classes, X_train, y_train, X_test, y_test)\r\n",
    "    '''\r\n",
    "\r\n",
    "    # Track the learning curve of the classifier, here we want the \r\n",
    "    # training and validation scores to approach 1\r\n",
    "    visualizer = LearningCurve(results[1], scoring='f1_weighted')\r\n",
    "    visualizer.fit(results[3], results[4])\r\n",
    "    visualizer.show()\r\n",
    "\r\n",
    "\r\n",
    "def visualize_learning_curve_test(results: tuple) -> None:\r\n",
    "    '''\r\n",
    "        Takes a 7-tuple from the run_experiments function and creates a learning curve\r\n",
    "\r\n",
    "        results: tuple = (name, model, classes, X_train, y_train, X_test, y_test)\r\n",
    "    '''\r\n",
    "\r\n",
    "    # Track the learning curve of the classifier, here we want the \r\n",
    "    # training and validation scores to approach 1\r\n",
    "    visualizer = LearningCurve(results[1], scoring='f1_weighted')\r\n",
    "    visualizer.fit(results[5], results[6])\r\n",
    "    visualizer.show()\r\n",
    "\r\n",
    "\r\n",
    "def visualize_confusion_matrix(results: tuple) -> None:\r\n",
    "    '''\r\n",
    "        Takes a 7-tuple from the run_experiments function and creates a confusion matrix\r\n",
    "\r\n",
    "        results: tuple = (name, model, classes, X_train, y_train, X_test, y_test)\r\n",
    "    '''\r\n",
    "\r\n",
    "    visualizer = yb.classifier.ConfusionMatrix(results[1], classes=results[2], title=results[0])\r\n",
    "    visualizer.score(results[5], results[6])\r\n",
    "    visualizer.show()\r\n",
    "\r\n",
    "\r\n",
    "def visualize_roc(results: tuple) -> None:\r\n",
    "    '''\r\n",
    "        Takes a 7-tuple from the run_experiments function and creates a \r\n",
    "        Receiver Operating Characteristic (ROC) Curve\r\n",
    "\r\n",
    "        results: tuple = (name, model, classes, X_train, y_train, X_test, y_test)\r\n",
    "    '''\r\n",
    "\r\n",
    "    visualizer = yb.classifier.ROCAUC(results[1], classes=results[2], title=results[0])\r\n",
    "    visualizer.score(results[5], results[6])\r\n",
    "    visualizer.poof()\r\n",
    "\r\n",
    "\r\n",
    "def visualize_pr_curve(results: tuple) -> None:\r\n",
    "    '''\r\n",
    "        Takes a 7-tuple from the run_experiments function and creates a \r\n",
    "        Precision-Recall Curve\r\n",
    "\r\n",
    "        results: tuple = (name, model, classes, X_train, y_train, X_test, y_test)\r\n",
    "    '''\r\n",
    "\r\n",
    "    visualizer = yb.classifier.PrecisionRecallCurve(results[1], title=results[0])\r\n",
    "    visualizer.score(results[5], results[6])\r\n",
    "    visualizer.poof()\r\n",
    "\r\n",
    "\r\n",
    "def visualize_report(results: tuple) -> None:\r\n",
    "    '''\r\n",
    "        Takes a 7-tuple from the run_experiments function and creates a report\r\n",
    "        detailing the Precision, Recall, f1, and Support scores for all \r\n",
    "        classification outcomes\r\n",
    "\r\n",
    "        results: tuple = (name, model, classes, X_train, y_train, X_test, y_test)\r\n",
    "    '''\r\n",
    "\r\n",
    "    visualizer = yb.classifier.ClassificationReport(results[1], classes=results[2], title=results[0], support=True)\r\n",
    "    visualizer.score(results[5], results[6])\r\n",
    "    visualizer.poof()\r\n",
    "\r\n",
    "\r\n",
    "def visualize_class_balance(results: tuple) -> None:\r\n",
    "    '''\r\n",
    "        Takes a 7-tuple from the run_experiments function and creates a histogram\r\n",
    "        detailing the balance between classification outcomes\r\n",
    "\r\n",
    "        results: tuple = (name, model, classes, X_train, y_train, X_test, y_test)\r\n",
    "    '''\r\n",
    "\r\n",
    "    visualizer = yb.target.ClassBalance(labels=results[0])\r\n",
    "    visualizer.fit(results[4], results[6])\r\n",
    "    visualizer.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A series of generators are composed to load and package the data for the experiments. Since the expressions used are all generators, the data is loaded only once it is used to run the experiment, afterwards it is eventually removed by the garbage collector."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# use the get_file_path closure to create a function that will return the path to a file\r\n",
    "baseline_path  = get_file_path(data_path_1)\r\n",
    "timebased_path = get_file_path(data_path_2)\r\n",
    "\r\n",
    "\r\n",
    "# create a list of the paths to all of the dataset files\r\n",
    "baseline_files : list = list(map(baseline_path , datasets))\r\n",
    "timebased_files: list = list(map(timebased_path, datasets))\r\n",
    "\r\n",
    "\r\n",
    "# then we create a generator that will return the dataframes for each dataset\r\n",
    "baseline_dfs : map = map(load_data   , baseline_files )\r\n",
    "timebased_dfs: map = map(load_data   , timebased_files)\r\n",
    "experiments  : zip = zip(baseline_dfs, timebased_dfs  , attacks)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our experiments are defined and packaged in the generator\r\n",
    "\r\n",
    "To run an experiment, we simply execute the expression: results = next(experiment)\r\n",
    "\r\n",
    "To run a specific experiment, we execute the expression: results = do_experiment[experiment_number]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def experiment_runner():\r\n",
    "    '''\r\n",
    "        A generator that handles running the experiments\r\n",
    "    '''\r\n",
    "    num = 1\r\n",
    "    for baseline, timebased, info in experiments:\r\n",
    "        print(f'Running experiment #{num}:\\t{info}')\r\n",
    "\r\n",
    "        print('Baseline results')\r\n",
    "        baseline_results = run_experiment(baseline, f'{info}_vs_ddos_baseline')\r\n",
    "        \r\n",
    "        print('\\nTime-based results')\r\n",
    "        timebased_results = run_experiment(timebased, f'{info}_vs_ddos_timebased')\r\n",
    "        \r\n",
    "        num += 1\r\n",
    "        yield (baseline_results, timebased_results, info, num)\r\n",
    "\r\n",
    "\r\n",
    "def do_experiment(num: int) -> tuple:\r\n",
    "    '''\r\n",
    "        A function that runs the specific experiment specified\r\n",
    "    '''\r\n",
    "    index = num - 1\r\n",
    "    baseline = load_data(baseline_files[index])\r\n",
    "    timebased = load_data(timebased_files[index])\r\n",
    "    info = attacks[index]\r\n",
    "\r\n",
    "    print(f'Running experiment #{num}:\\t{info}')\r\n",
    "\r\n",
    "    print('Baseline results')\r\n",
    "    baseline_results = run_experiment(baseline, f'{info}_vs_all_baseline')\r\n",
    "    \r\n",
    "    print('\\nTime-based results')\r\n",
    "    timebased_results = run_experiment(timebased, f'{info}_vs_all_timebased')\r\n",
    "\r\n",
    "    return (baseline_results, timebased_results, info, num)\r\n",
    "\r\n",
    "\r\n",
    "experiment = experiment_runner()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiments\r\n",
    "\r\n",
    "Our experiments are now conducted, the results are taken and are used to create a series of visualizations each representing various aspects of the experiments. \r\n",
    "\r\n",
    "The Baseline results are plotted followed by the Timebased results for each graph\r\n",
    "\r\n",
    "the variable results holds the references to all the datasets being used by the current experiment, once the next experiment is executed, the results variabe is overwritten and the references to the previous datasets are removed. The previous datasets are then eventually removed from memory by the garbage collector."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #1: DNS vs DDoS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Dataset: ../data/downsampled/baseline/DNS_vs_ddos_200000.csv\n",
      "\tTo Dataset Cache: ../data/cache/baseline/DNS_vs_ddos_200000.csv.pickle\n",
      "\n",
      "Loading Dataset: ../data/downsampled/timebased/DNS_vs_ddos_200000.csv\n",
      "\tTo Dataset Cache: ../data/cache/timebased/DNS_vs_ddos_200000.csv.pickle\n",
      "\n",
      "Running experiment #1:\tDNS\n",
      "Baseline results\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_learning_curve_test(results[Baseline])\r\n",
    "visualize_learning_curve_test(results[Timebased])\r\n",
    "visualize_report(results[Baseline])\r\n",
    "visualize_report(results[Timebased])\r\n",
    "visualize_confusion_matrix(results[Baseline])\r\n",
    "visualize_confusion_matrix(results[Timebased])\r\n",
    "visualize_roc(results[Baseline])\r\n",
    "visualize_roc(results[Timebased])\r\n",
    "visualize_pr_curve(results[Baseline])\r\n",
    "visualize_pr_curve(results[Timebased])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #2: LDAP vs DDoS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_learning_curve_test(results[Baseline])\r\n",
    "visualize_learning_curve_test(results[Timebased])\r\n",
    "visualize_report(results[Baseline])\r\n",
    "visualize_report(results[Timebased])\r\n",
    "visualize_confusion_matrix(results[Baseline])\r\n",
    "visualize_confusion_matrix(results[Timebased])\r\n",
    "visualize_roc(results[Baseline])\r\n",
    "visualize_roc(results[Timebased])\r\n",
    "visualize_pr_curve(results[Baseline])\r\n",
    "visualize_pr_curve(results[Timebased])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #3: MSSQL vs DDoS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_learning_curve_test(results[Baseline])\r\n",
    "visualize_learning_curve_test(results[Timebased])\r\n",
    "visualize_report(results[Baseline])\r\n",
    "visualize_report(results[Timebased])\r\n",
    "visualize_confusion_matrix(results[Baseline])\r\n",
    "visualize_confusion_matrix(results[Timebased])\r\n",
    "visualize_roc(results[Baseline])\r\n",
    "visualize_roc(results[Timebased])\r\n",
    "visualize_pr_curve(results[Baseline])\r\n",
    "visualize_pr_curve(results[Timebased])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #4: NetBIOS vs DDoS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_learning_curve_test(results[Baseline])\r\n",
    "visualize_learning_curve_test(results[Timebased])\r\n",
    "visualize_report(results[Baseline])\r\n",
    "visualize_report(results[Timebased])\r\n",
    "visualize_confusion_matrix(results[Baseline])\r\n",
    "visualize_confusion_matrix(results[Timebased])\r\n",
    "visualize_roc(results[Baseline])\r\n",
    "visualize_roc(results[Timebased])\r\n",
    "visualize_pr_curve(results[Baseline])\r\n",
    "visualize_pr_curve(results[Timebased])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #5: NTP vs DDoS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# results = next(experiment)\r\n",
    "results = do_experiment(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_learning_curve_test(results[Baseline])\r\n",
    "visualize_learning_curve_test(results[Timebased])\r\n",
    "visualize_report(results[Baseline])\r\n",
    "visualize_report(results[Timebased])\r\n",
    "visualize_confusion_matrix(results[Baseline])\r\n",
    "visualize_confusion_matrix(results[Timebased])\r\n",
    "visualize_roc(results[Baseline])\r\n",
    "visualize_roc(results[Timebased])\r\n",
    "visualize_pr_curve(results[Baseline])\r\n",
    "visualize_pr_curve(results[Timebased])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #6: Portmap vs DDoS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_learning_curve_test(results[Baseline])\r\n",
    "visualize_learning_curve_test(results[Timebased])\r\n",
    "visualize_report(results[Baseline])\r\n",
    "visualize_report(results[Timebased])\r\n",
    "visualize_confusion_matrix(results[Baseline])\r\n",
    "visualize_confusion_matrix(results[Timebased])\r\n",
    "visualize_roc(results[Baseline])\r\n",
    "visualize_roc(results[Timebased])\r\n",
    "visualize_pr_curve(results[Baseline])\r\n",
    "visualize_pr_curve(results[Timebased])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #7: SNMP vs DDoS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_learning_curve_test(results[Baseline])\r\n",
    "visualize_learning_curve_test(results[Timebased])\r\n",
    "visualize_report(results[Baseline])\r\n",
    "visualize_report(results[Timebased])\r\n",
    "visualize_confusion_matrix(results[Baseline])\r\n",
    "visualize_confusion_matrix(results[Timebased])\r\n",
    "visualize_roc(results[Baseline])\r\n",
    "visualize_roc(results[Timebased])\r\n",
    "visualize_pr_curve(results[Baseline])\r\n",
    "visualize_pr_curve(results[Timebased])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #8: SSDP vs DDoS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_learning_curve_test(results[Baseline])\r\n",
    "visualize_learning_curve_test(results[Timebased])\r\n",
    "visualize_report(results[Baseline])\r\n",
    "visualize_report(results[Timebased])\r\n",
    "visualize_confusion_matrix(results[Baseline])\r\n",
    "visualize_confusion_matrix(results[Timebased])\r\n",
    "visualize_roc(results[Baseline])\r\n",
    "visualize_roc(results[Timebased])\r\n",
    "visualize_pr_curve(results[Baseline])\r\n",
    "visualize_pr_curve(results[Timebased])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #9: Syn vs DDoS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_learning_curve_test(results[Baseline])\r\n",
    "visualize_learning_curve_test(results[Timebased])\r\n",
    "visualize_report(results[Baseline])\r\n",
    "visualize_report(results[Timebased])\r\n",
    "visualize_confusion_matrix(results[Baseline])\r\n",
    "visualize_confusion_matrix(results[Timebased])\r\n",
    "visualize_roc(results[Baseline])\r\n",
    "visualize_roc(results[Timebased])\r\n",
    "visualize_pr_curve(results[Baseline])\r\n",
    "visualize_pr_curve(results[Timebased])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #10: TFTP vs DDoS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_learning_curve_test(results[Baseline])\r\n",
    "visualize_learning_curve_test(results[Timebased])\r\n",
    "visualize_report(results[Baseline])\r\n",
    "visualize_report(results[Timebased])\r\n",
    "visualize_confusion_matrix(results[Baseline])\r\n",
    "visualize_confusion_matrix(results[Timebased])\r\n",
    "visualize_roc(results[Baseline])\r\n",
    "visualize_roc(results[Timebased])\r\n",
    "visualize_pr_curve(results[Baseline])\r\n",
    "visualize_pr_curve(results[Timebased])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #11: UDP vs DDoS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = next(experiment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_learning_curve_test(results[Baseline])\r\n",
    "visualize_learning_curve_test(results[Timebased])\r\n",
    "visualize_report(results[Baseline])\r\n",
    "visualize_report(results[Timebased])\r\n",
    "visualize_confusion_matrix(results[Baseline])\r\n",
    "visualize_confusion_matrix(results[Timebased])\r\n",
    "visualize_roc(results[Baseline])\r\n",
    "visualize_roc(results[Timebased])\r\n",
    "visualize_pr_curve(results[Baseline])\r\n",
    "visualize_pr_curve(results[Timebased])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment #12: UDP-lag vs DDoS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# results = next(experiment)\r\n",
    "results = do_experiment(12)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_learning_curve_test(results[Baseline])\r\n",
    "visualize_learning_curve_test(results[Timebased])\r\n",
    "visualize_report(results[Baseline])\r\n",
    "visualize_report(results[Timebased])\r\n",
    "visualize_confusion_matrix(results[Baseline])\r\n",
    "visualize_confusion_matrix(results[Timebased])\r\n",
    "visualize_roc(results[Baseline])\r\n",
    "visualize_roc(results[Timebased])\r\n",
    "visualize_pr_curve(results[Baseline])\r\n",
    "visualize_pr_curve(results[Timebased])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "assert False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Re-Execution of Experiments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = do_experiment(12)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_report(results[Baseline])\r\n",
    "visualize_report(results[Timebased])\r\n",
    "visualize_roc(results[Baseline])\r\n",
    "visualize_roc(results[Timebased])\r\n",
    "visualize_pr_curve(results[Baseline])\r\n",
    "visualize_pr_curve(results[Timebased])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}